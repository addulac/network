\documentclass[a4paper, 12pt]{article}

%\usepackage[cmex10]{amsmath, mathtools}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{multirow}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{url}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{fancyvrb}
\usepackage{yfonts}
\usepackage{dsfont}
\usepackage{calc} %    For the \widthof macro
\usepackage{xparse} %  For \NewDocumentCommand
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{lipsum}
%\input{../tikz.conf}

\usepackage{graphicx}
\usepackage{subcaption}

\usetikzlibrary{bayesnet}

%%%%%%%%%%%%%%%%%%%%
%%% Goemetry
%%%%%%%%%%%%%%%%%%%%
%\usepackage[margin=0.25in]{geometry}
\usepackage{geometry}
\geometry{
    a4paper,
 total={420pt,700pt},
 %left=20mm,
 top=20mm,
 }


\title{Networks Properties -- Experiments}

\begin{document}

\maketitle
\tableofcontents

To illustrate our theoretical results, we evaluate the predictive performance and the ability of the models to capture homophily and preferential attachment on artificial and real networks. In the sequel, we first describe the measures used to evaluate the properties of interest and the predictive performance, then the datasets used in our experiments. Then, we detail the evaluation protocol and we present the experimental results.

\section{Evaluation Measures}
This article focus on two properties of networks, namely the preferential attachment effect and the homophily. In this section, we present the measures used in our experiments in order to characterized these properties and to evaluate the predictive performance of the models.

\subsection{Measures for the properties}

\subsubsection{Preferential attachment}
\label{sec:experiments-burst}

The measures considered to evaluate the preferential attachment rely on a goodness of fit. Indeed, it has been reported that preferential attachment leads to networks characterized by a degree distribution with heavy tail \textbf{ref?}. A typical form of such law, often meet in data, is a a power law distribution. The comparison of the degree distribution in the log-log scale with a linear function gives us a qualitative measure for the preferential attachment. To obtain a second evaluation of the power law hypothesis for the degree distribution, we rely on a  goodness a fit based on a Kolmogorov-Smirnov (KS) test. We follow the protocol described in \cite{clauset2009power} which consists of the following steps:
\begin{itemize}
	\item Estimate the parameters $x_\text{min}$ and $\alpha$ of the power law model. Nevertheless, in order to provide a comparable measure of different degrees distributions, as it is the case in our experimentations, we choose to fix $x_\text{min}$  to the smallest value observed in the degree distribution evaluated.
	\item Calculate the goodness of fit on the data sample (typically the degree of a networks). The resulting $p$-value gives an estimate of the  plausibility of the hypothesis for the data. The p-value is actually the ratio of the number of the time where a kolmogorov-Smirnov test statistics, evaluated on samples generated by the power law, is higher than the KS test statistics on the data samples. 
    \item the number of times $S$ where the KS statistics is compared is chosen, following \cite{clauset2009power}, with a precision of $\epsilon = 3^{-2}$. The  number of loops is then $S = \frac{1}{4}\epsilon^{-2}$.
\end{itemize}

If p is large (close to 1), then the difference between the data and the model can be attributed to statistical fluctuations alone; if it is small, the model is not a plausible fit for the data and we can not conclude that there is an evidence for the preferential attachment in the network. However, as mentioned in \cite{clauset2009power} high value of the $p$-value should be considered with caution for at least two reasons. First, there may be other distribution that match the data equally or better. Second, a small number of samples of the data may lead to high p-value and reflect the fact that is hard to rule out a hypothesis in such a case.

\subsubsection{Homophily}

For the homophily property, defined in section \ref{sec:homophily}, we rely on two different measures:

\begin{itemize}
    \item  Pearson' correlation coefficient between the probability of having a link $P(y_{ij}=1),  i, j \in E^2 $, and the  \textbf{latent / natural ?} similarity $(s_{ij})$ for all pairs of vertices. 
    \item  Means and standard deviations of the similarity computed respectively on the linked and non-linked pairs of nodes.
\end{itemize}

\subsection{Prediction Performance}
The prediction problem is equivalent to a binary prediction problem in two classes since it consists to decide for each pair of nodes if there is an edge or not between them. 
Thus, the performance of the models can be evaluated with usual measures:

\begin{itemize}

\item Precision/Recall/$F_1$ :  The local precision, recall and $F_1$ scores concern the links predicted as an edge.  The global (precision), denoted global in the tables, refers to the accuracy of the model. 
\textit{mettre ailleurs} Note that each group of row is indexed by  a number $K$ which indicates various values of initialization of the latent feature dimension.
\item AUC-ROC : Receiver operating characteristic curves allow also to graphically compare the  models on each dataset.
\end{itemize}

\section{Training Datasets}

In our experiments, we consider four artificial networks and two real networks.

\subsection{Artificial networks}

The artificial networks have been generated with DANC-Generator \cite{largeron2015}. This generator has been chosen because it allows to build an attributed graph having a community structure  and  the known properties of real-world networks such as preferential attachment and homophily.
Moreover, by modifying the parameters, these properties can be weakened. Finally, DANC-Generator is available under the terms of the GNU Public License and the        parameters can be shared for experiments reproducibility.

Four artificial networks (Network1, ..., Network4) have been generated.  Their adjacency matrices and global degree distributions are presented in Figure \ref{fig:synt_graph}. Table \ref{table:synt_graph} gives the results of the KS test: $p$-value, values estimated for the parameters  $\alpha$ and $x_\text{min}$,  and $n_{tail}$ \textbf{ntail ?}. For each network, Figure \ref{fig:synt_graph_local} represents the local degree distribution associated to each \emph{ground truth} community and Table \ref{table:synt_graph_local} reports the KS results in the local case. The inner degree distribution (edges inside a community) and outer degree distribution (edges between community) are plotted separately.

Each network presents a different affinity to preferential attachment and homophily.
Indeed, As shown Figure \ref{fig:synt_graph}, Network1 and Network2 , and to a lesser extend, Network3 verify preferential attachment whereas it is not the case for the Network3. The results of the KS test in Table \ref{table:synt_graph} confirm this analysis since the $p$-value equals 1 for Network1 and Network2 and is equal to 0 for Network4.
Concerning the local preferential attachment, that can be evaluated on these artificial networks where the ground truth communities are provided by the generator, the same conclusions can be drawn from the Figure \ref{fig:synt_graph_local} and the Table \ref{table:synt_graph_local}.

\input{t/a.tex}
\input{t/b.tex}

\begin{table}[h]
\caption{Power law goodness of fit results for characterization of global preferential attachment in synthetic networks.}
\centering
    \begin{tabular}{lrrrr}
    \hline
               &   pvalue &   alpha &   x\_min &   n\_tail \\
    \hline
     Network1 &    1.000 &   2.424 &       3 & 1000.000 \\
     Network2 &    0.971 &   2.897 &       3 & 1000.000 \\
     Network3 &    0.798 &   1.758 &       3 & 1000.000 \\
     Network4 &    0.000 &   1.354 &       3 & 1000.000 \\
    \hline
    \end{tabular}
\label{table:synt_graph}
\end{table}

\begin{table}[h]
\caption{Power law goodness of fit results for characterization of local preferential attachment in synthetic networks.}
\centering
    \begin{tabular}{lllll}
    \hline
    & pvalue          & alpha           & x\_min        & n\_tail           \\
    \hline
    Network1 & 0.9 $\pm$ 0.07  & 2.7 $\pm$ 0.1 & 1.8 $\pm$ 0.9 & 154.4 $\pm$ 83.9 \\
    Network2 & 0.9 $\pm$ 0.008 & 3.0 $\pm$ 1.2  & 1.8 $\pm$ 0.9 & 170.9 $\pm$ 66.4  \\
    Network3 & 0.8 $\pm$ 0.26 & 7.0 $\pm$ 5.8 & 1.8 $\pm$ 0.9 & 136.3 $\pm$ 94.0 \\
    Network4 & 0.6 $\pm$ 0.49    & 1.5 $\pm$ 0.1 & 1.8 $\pm$ 0.9 & 204.7 $\pm$ 53.0 \\
    \hline
    \end{tabular}
\label{table:synt_graph_local}
\end{table}

\subsection{Real networks}

We evaluate also the models on two real networks.
The first one, denoted UC Irvine \footnote{available at:}, is built from a online community of 1899 students from the University of California. Each node corresponds to a user and a   directed edge represents a sent message.
The second one, denoted Manufacturing \footnote{available at:}, is an internal email communication network between employees of a mid-sized manufacturing company. Each vertex is associated  to an employee and an oriented link represents like previously a sent email.

The adjacency matrices and global degree distributions are presented in Figure \ref{fig:real_graph}. The goodness of fit based on the KS test, used as a reference for the global preferential attachment effect, is reported in Table \ref{table:real_graph}. According to Figure \ref{fig:real_graph} as well as Table \ref{table:real_graph}, it appears that the preferential attachment property is verified in Manufacturing and not in UC Irvine.

\input{t/c.tex}

\begin{table}
\caption{Power law goodness of fit results for characterization of global degree attachment in real networks.}
\centering
\begin{tabular}{lrrrr}
   &   pvalue &   alpha &   x\_min &   n\_tail \\
\hline
 Manufacturing &    0.000 &   1.434 &       3 &  167.000 \\
 UC Irvine     &    0.989 &   1.787 &       3 & 1899.000 \\
\hline
\end{tabular}
\label{table:real_graph}
\end{table}

Finally Table \ref{table:networks_measures} summarizes some other properties characteristics of these artificial and real networks. 

\subsection{Standard Measures}
\textcolor{red}{Temporally section, need discussion}

Here is a  table where we reported  some standard measure on networks. 
\textcolor{red}{ Is homophily (from Dancer, with the Kleinberg measure, $H_{obs} - H_{exp}$)  standard ? }

It is important to note that each measure is not decorrelated from each others. Are we sure that we can answer the following question ? : ~\\

\hspace{1cm}\textit{"Are the burstinesses effects really decorrelated from other properties (clustering, sparstity), to conclude on the fact that a model perform better than a other because of this particular properties (ie the global or local burstiness) ? (\textbf{Correlation \emph{vs} Causality}) } (the same questions holds for the impact that each properties have on each other. ) 

\begin{table}[h] 
\centering
	\caption{table:Networks standard measure on datasets.}
\resizebox{\textwidth}{!}{  
	\begin{tabular}{lcrrrrrr}
		\hline
		Networks   &  \small{homophily} ($H_ {obs}$ ; $\Delta_{obs - exp}$)    &  Modularity & \small{Clustering coefff} & density  & Nodes & edges & \small{diameter}   \\
		\hline
		Network1      & 0.561 ;  0.08 & 0.59 & 0.06 & 0.007 & 1000  & 6014  & 8 \\
		Network2      & 0.429 ; -0.04 & 0.43 & 0.08 & 0.006 & 1000  & 5000 & 10 \\
		Network3      & 0.439 ; -0.03 & 0.71 & 0.49 & 0.01  & 1000  & 11000 &8 \\
		Network4      & 0.621 ;  0.18 & 0.68 & 0.61 & 0.06  & 1000  & 61000  & 5 \\
		manufacturing & --            & --   & 0.59 & 0.24  & 167  & 5950 & 5 \\
		UC Irvine     & --            & --   & 0.10 & 0.08  & 1899 & 22195  & -- \\
		\hline
	\end{tabular}
}
\label{table:networks_measures}
\end{table}

\subsubsection{Comments}

\paragraph{Note on the goodness of fit results for synthetic networks:}
\begin{itemize}
    \item Network 1 to 4 have a decreasing pvalue in their overall degree distribution. Network 1 and 2 have a relatively high pvalue that suggests a high confidence in a power law fit. In the opposite, Network 4 reject the power law hypothesis strongly with a null pvalue.
    \item The analysis on local degree distribution show that network has high pvalue with a small standard deviation, while network 3 and 4 have lower pvalue and higher standard deviation which suggest that they satisfy poorly the local preferential attachment.
\end{itemize}


\section{$M_e$ -- Model fitted}

For each datasets described earlier, we run a MCMC inference consisting of 200 iterations to learn the posterior distribution of each the IMMSB model and ILFM, described in \ref{sec:models}. For IMMSB, concentration parameters of HDP were optimized following \cite{HDP} using vague gamma priors $\alpha_0 \sim \text{Gamma}(1,1)$ and $\gamma \sim \text{Gamma}(1,1)$. The parameter for the matrix weights were fixed to $\lambda_0=\lambda_1=0.1$. For ILFM, the IBP hyper-parameter was fixed to $\alpha=0.5$ and the weights hyper-parameter to $\sigma_w = 1$. 

The inference procedure was run under this settings in all of the 4 synthetic datasets and 2 real networks.

All our experimental platform is available online \footnote{https://github.com/dtrckd/pymake}. It is an ongoing development in order to provide a flexible way to design and run experiments and make data analysis.

\subsection{Burstiness}

In order to measure the different level of burstiness we used the models to generate full networks. (The procedure for such measure is similar than those explain in the section \ref{sec:mgmg}. Thus given the model parameters $\mathcal{M}_e = \{F ,\Phi\}$, we generate a set of 20 networks. In the next, results are averaged  with standard deviation for all those generated networks.

% Global
In figure \ref{fig:me_fit_gburst_mmsb} and \ref{fig:me_fit_gburst_ibp}, we report the global degree for generated networks for respectively IMMSB and ILFM. In Table \ref{table:global_gof}, we report the corresponding goodness of fit evaluation.


\input{t/me_fit_gburst_mmsb}
\input{t/me_fit_gburst_ibp}

\begin{table}
    \caption{Power law Goodness of fit for the global preferential attachment effect.}
\centering
    \begin{tabular}{lllll}
    \hline
        \textbf{IMMSB} & pvalue          & alpha           & x\_min          & n\_tail           \\
    \hline
    Network1     & 0.907 $\pm$ 0.11 & 1.396 $\pm$ 0.004 & 1.0 $\pm$ 0.0    & 990.1 $\pm$ 3.1  \\
    Network2     & 1.0 $\pm$ 0.0     & 1.452 $\pm$ 0.005 & 1.0 $\pm$ 0.0    & 974.4 $\pm$ 4.9  \\
    Network3     & 0.0 $\pm$ 0.0     & 1.294 $\pm$ 0.001 & 1.0 $\pm$ 0.0    & 999.0 $\pm$ 0.9 \\
    Network4     & 0.0 $\pm$ 0.0     & 1.203 $\pm$ 0.025 & 1.25 $\pm$ 0.5 & 999.1 $\pm$ 0.7 \\
    Manufacturing & 0.006 $\pm$ 0.01 & 1.244 $\pm$ 0.002 & 1.0 $\pm$ 0.0    & 165.3 $\pm$ 1.3 \\
    UC Irvine     & 1.0 $\pm$ 0.0     & 1.348 $\pm$ 0.002 & 1.0 $\pm$ 0.0    & 1852.7 $\pm$ 5.7 \\
    \hline
    \end{tabular}

    \begin{tabular}{lllll}
    \hline
        \textbf{ILFM} & pvalue          & alpha           & x\_min       & n\_tail           \\
    \hline
    Network1     & 1.0 $\pm$ 0.0     & 1.4 $\pm$ 0.003 & 1.0 $\pm$ 0.0 & 919.5 $\pm$ 5.9 \\
    Network2     & 1.0 $\pm$ 0.0     & 1.4 $\pm$ 0.004 & 1.0 $\pm$ 0.0 & 893.7 $\pm$ 7.1  \\
    Network3     & 0.01 $\pm$ 0.02  & 1.3 $\pm$ 0.002 & 1.0 $\pm$ 0.0 & 982.7 $\pm$ 3.6 \\
    Network4     & 0.0 $\pm$ 0.0     & 1.2 $\pm$ 0.018 & 1.1 $\pm$ 0.3 & 998.4 $\pm$ 0.9 \\
    Manufacturing & 0.018 $\pm$ 0.01 & 1.2 $\pm$ 0.002 & 1.0 $\pm$ 0.0 & 164.2 $\pm$ 1.4 \\
    UC Irvine     & 1.0 $\pm$ 0.0     & 1.3 $\pm$ 0.002 & 1.0 $\pm$ 0.0 & 1798.3 $\pm$ 8.2 \\
    \hline
    \end{tabular}
    \label{table:global_gof}
\end{table}



% Local
In figure \ref{fig:me_fit_lburst_mmsb} and \ref{fig:me_fit_lburst_ibp}, we report the local degree for generated networks for respectively IMMSB and ILFM. In Table \ref{table:local_gof}, we report the corresponding goodness of fit evaluation. \underline{Note that the pvalue reported here, are the averaged of all} classes.


\textcolor{red}{here comment...} ~\\
\textcolor{red}{x\_min could be removed from table, I explains earlier how I choose it.}

\input{t/me_fit_lburst_mmsb}
\input{t/me_fit_lburst_ibp}


\begin{table}
    \caption{Power law Goodness of fit for the local preferential attachment effect.}
\centering
    \begin{tabular}{lllll}                                                                                    
    \hline                                                                
        \textbf{IMMSB}  & pvalue         & alpha           & x\_min          & n\_tail             \\            
    \hline                                                                                                    
    Network1     & 1.0 $\pm$ 0.0    & 7.433 $\pm$ 2.2 & 1.0 $\pm$ 0.0    & 53.737 $\pm$ 9.8   \\
    Network2     & 1.0 $\pm$ 0.0    & 1 $\pm$ 0.0    & 1.0 $\pm$ 0.0    & 41.895 $\pm$ 11.2  \\                             
    Network3     & 1.0 $\pm$ 0.0    & 4.074 $\pm$ 0.7 & 1.0 $\pm$ 0.0    & 112.143 $\pm$ 18.5 \\                
    Network4     & 1.0 $\pm$ 0.0    & 1.957 $\pm$ 0.2 & 1.0 $\pm$ 0.0    & 316.095 $\pm$ 69.5 \\
    Manufacturing & 0.55 $\pm$ 0.5 & 1.242 $\pm$ 1.1 & 0.55 $\pm$ 0.4 & 31.4 $\pm$ 14.644    \\              
    UC Irvine     & 1.0 $\pm$ 0.0    & 5.213 $\pm$ 0.9 & 1.0 $\pm$ 0.0    & 242.0 $\pm$ 26.1   \\                
    \hline                                                                
    \end{tabular}    

    \begin{tabular}{lllll}
    \hline
        \textbf{ILFM} & pvalue          & alpha           & x\_min           & n\_tail              \\
    \hline
    Network1     & 1.0 $\pm$ 0.0     & 1.93 $\pm$ 0.3  & 1.0 $\pm$ 0.0     & 325.7 $\pm$ 154.9 \\
    Network2     & 0.952 $\pm$ 0.2 & 1.76 $\pm$ 0.5 & 0.9 $\pm$ 0.2 & 423.8 $\pm$ 210.1  \\
    Network3     & 0.957 $\pm$ 0.2 & 1.58 $\pm$ 0.3 & 1.0 $\pm$ 0.0     & 587.0 $\pm$ 252.2 \\
    Network4     & 0.698 $\pm$ 0.4 & 1.33 $\pm$ 0.1 & 1.0 $\pm$ 0.0     & 632.9 $\pm$ 200.1  \\
    Manufacturing & 0.69 $\pm$ 0.3   & 1.40 $\pm$ 0.2 & 1.0 $\pm$ 0.0     & 92.0 $\pm$ 26.5     \\
    UC Irvine     & 1.0 $\pm$ 0.0     & 1.87 $\pm$ 0.4   & 1.0 $\pm$ 0.0     & 994.3 $\pm$ 408.8 \\
    \hline
    \end{tabular}
\label{table:local_gof}
\end{table}

\subsubsection{Comments}

\begin{itemize}
    \item global preferential attachment : both IMMSB and ILFM pass the goodness of fit test for networks 1,2 and UC irvine. The 3 other are rejected. This show that the models can fit to strongly bursty networks. When burstiness is less strong, as for network3, we can notice if we consider the tail of the generated distributions, ILFM appears to have straight tail than ILFM.
    \item local preferential attachment : We can see that the fitness for the local degree distributions of IMMSB is strong for every networks except in the lesser extent for Manufacturing. The confident is also relatively high because of a very small variance over the different class, except again for manunufacturing. The instability of Manufacturing is probably due to his small number of nodes. For ILFM, the fitness decrease from network1 to network4. Also, the pvalue has high variance for network 2 to 4 and for manufacturing which is correlated with the decay of the global burstiness.
\end{itemize}

\textcolor{red}{here comment...} 

\subsection{Homophily}

%We evaluate in this section the behaviour of homophily of models according to both, the natural and the latent similarity, reported respectively in tables \ref{table:homo_natural} and \ref{table:homo_latent}. Those results are obtained by computing the Pearson correlation coefficient between a random adjacency matrix and a similarity matrix matrix as defined in \ref{sec_homophily} from a model (either IMMSB or ILFM). Each of these coefficient is averaged an basis on 20 networks generated from the fitted models.

%Firstly, we can notice that the results are stable since the standard deviation is not significant for all experiments. Secondly those results illustrate the fact that both models are homophilic under the natural similarity with a positive correlation in all networks,  and not under the latent similarity with a correlation coefficient near to 0 and oscillating between positive and negative value (Note that this last fact depend on the initialization and the seed of the models).

%%%% correlation betewwen edges and similarity
%\begin{table}
%\caption{Correlation between the natural similarity matrix and adjacency matrix for IMMSB and ILFM.}
%\centering
%    \begin{tabular}{lll}
%    \hline
%    & IMMSB   & ILFM  \\
%    \hline
%    Network1     & 0.23 $\pm$ 0.004  & 0.135 $\pm$ 0.001      \\
%    Network2     & 0.225 $\pm$ 0.005 & 0.112 $\pm$ 0.001      \\
%    Network3     & 0.186 $\pm$ 0.002 & 0.165 $\pm$ 0.001      \\
%    Network4     & 0.385 $\pm$ 0.001 & 0.443 $\pm$ 0.0        \\
%    Manufacturing & 0.661 $\pm$ 0.003 & 0.583 $\pm$ 0.002      \\
%    UC Irvine     & 0.21 $\pm$ 0.002  & 0.129 $\pm$ 0.001      \\
%    \hline
%    \end{tabular}
%\label{table:homo_natural}
%\end{table}
%
%
%\begin{table}
%    \caption{Correlation between the latent similarity matrix and adjacency matrix for IMMSB and ILFM.}
%    \begin{tabular}{lll}
%    \hline
%    & ILFM  & IMMSB \\
%    \hline
%    Network1     & 0.021 $\pm$ 0.001  & -0.035 $\pm$ 0.001      \\
%    Network2     & -0.007 $\pm$ 0.001 & -0.051 $\pm$ 0.001      \\
%    Network3     & 0.012 $\pm$ 0.001  & 0.002 $\pm$ 0.001       \\
%    Network4     & 0.141 $\pm$ 0.001  & 0.056 $\pm$ 0.001       \\
%    Manufacturing & 0.11 $\pm$ 0.003   & 0.073 $\pm$ 0.004       \\
%    UC Irvine     & 0.148 $\pm$ 0.002  & -0.063 $\pm$ 0.0        \\
%    \hline
%    \end{tabular}
%\label{table:homo_latent}
%\end{table}


We evaluate in this section the behaviour of homophily of models according to both, the natural and the latent similarity.

In table \ref{table:homo_pearson} we compute the Pearson correlation for the homophily measure. Note that this coefficient for the natural similarity is always equal to one.

In table \ref{table:homo_natural} and \ref{table:homo_latent} we compute contingency tables for homophily.
\textcolor{red}{ces deux tablueaux sont horrible a formater. Je les laisse tel quel pour le moemnt que tu vois les valeurs, masi je ne pense que ce soit la mesure à retenir. A la rigueur un average sur tout les network de cess valeurs...}

\input{t/table_homo_natural}
\input{t/table_homo_latent}

\begin{table}[h]
    \caption{Correlation between the latent similarity matrix and the predictive likelihood for IMMSB and ILFM.}
    \centering
    \begin{tabular}{lll}
    \hline
    & ILFM  & IMMSB \\
    \hline
    Network 1     & 0.082 &  -0.126   \\
    Network 2     & 0.001     &  -0.116   \\
    Network 3     & 0.114     &  0.123    \\
    Network 4     & 0.076     &  0.009    \\
    Manufacturing & 0.384     &  -0.011   \\
    UC Irvine     & 0.286     &  -0.213   \\
    \hline
    \label{table:homo_pearson}
    \end{tabular}
\end{table}


\subsubsection{Comments}
\begin{itemize}
    \item The measure of the natural similarity over links and non links satisfy the homophily effect for IMMSB and ILFM (table \ref{table:homo_natural_ibp} and \ref{table:homo_latent_immsb}). In the contrary there is no evidence for a correlation between the latent similarity and the link likelihood (table \ref{homo_pearson}. 
\end{itemize}




\subsection{Prediction Performance}
In order to evaluate the  prediction performance of the models, we build a training set and a testing set from the original datasets. In order to achieve this, we generate a random mask consisting of 20 percent of the size of the adjacency matrix. This mask is used a our testing set, while the 80 remaining percent are used as a training set. The results for the prediction evaluation are reported in i) the AUC-ROC measures in figure \ref{fig:auc}, and ii) in the precision/recall results in table \ref{table:unbalanced}.
Each prediction results were averaged on 10 repetitions, and results were found to be stable regarding on the random  initialization and the stochastic optimization procedure. 

\input{t/table_results}

\input{t/auc}

\subsubsection{Comments}
\textcolor{red}{here comment...} 

\section{$M_e$ -- Model Generated}
\label{sec:mgmg}

\subsection{Burstiness}

\textcolor{red}{Je changerais la forme (comment les groupe de figure sont plotté) selon ce que lon conviendra de conserver dan le partie expé.}

We illustrate the burstiness results by generating 3 random networks for each of both models. We report it in figure \ref{fig:gen_burst}. We use the 3 same settings for IMMSB  than those in the previous section. For ILFM  we use the following 3 settings: (column 1:$\alpha=1,  \lambda_1=\lambda_2=1$ , column 2:$\alpha=0.1, \lambda_1=\lambda_2=1$, column 3: $\alpha=1, \lambda_1=\lambda_2=10$) and we fix $\sigma_w=1$ and again $N=1000$.

Each row of the two sets of figures represents the three following measures who corresponds respectively to the global preferential attachment, the local preferential attachment and the feature burstiness:
\begin{itemize}
    \item First row: we measure the overall degree distributions; We report the average and standard deviation of the degree distribution in a linear scale for each generated network.
    \item Second row: we measure the local degree distribution on a log-log scale:
        \begin{itemize}
            \item for IMMSB, each network has an associated membership tensor $Z$, that indicates the membership of nodes for each  interactions. In order to draw the local degree distribution for a block $c$, we reduce the adjacency matrix in order to retain only the links that occurs inside a block $c$. The local degree distribution is thus computed on the reduce adjacency matrix $Y_c = Y \otimes (Z_i^c \times Z_j^c)$ where $\otimes$ is the hadamard product and $\times$ the outer product. $Z_i^c$ is the membership matrix of a nodes $i$ such that $Z_{ik}^c=0$ if $k\neq c$ (links that occurs outside $c$ will be ignored).
            \item for ILFM, each node is associated with a fix feature vector; the local degree distribution for the block $c$ is obtain by taking only the contribution of the features $c$ on the adjacency matrix. Thus, the local degree degree distribution is computed on the reduce adjacency matrix $Y_c = Y \otimes (F_{.c}\times F_{.c})$. 
        \end{itemize}
    \item Third row measure the distribution of the block membership, which directly reflect the feature burstiness. Hence for IMMSB, for a feature $k$, we measure $\sum_{ij} \mathds{1}(Z_{i\rightarrow j} = k, Z_{i\leftarrow j} = k)$ which indicates the number of nodes who were associated to the block $k$. For ILFM the number of nodes associated to a block $k$ is simply $\sum_n F_{nk}$.
\end{itemize}


% not sure it use significantly usefull, because p-value are either 0 or 1 ...
%We provide in annexe (\textcolor{red}{not yet included}),  an goodness of fit for all local degrees distribution that we plotted, in order to quantify the power law hypothesis on the empirical degree distributions. The number of feature is typically too small to evaluate an significant goodness of fit. The protocol is described in section \ref{sec:experiments-busrt}. 

The simulation shows us the following facts about the properties of the models:
\begin{itemize}
	\item For both model the global degree distribution don't exhibit a bursty phenomenon,
	\item The local degree distribution show that IMMSB exhibit a bursty phenomenon which is not the case for ILFM,
	\item Both models exhibits a bursty phenomenon on the feature distribution. This distributions for ILFM has a long tail for all of the 3 settings while for IMMSB, the tail shape is more or less long depending of the $\alpha$ and $\gamma$ parameters.
\end{itemize}

For the case of the global preferential attachment we have no formal proof for the non-compliance of the models, due the non closed form of the evidence in both models, due to the admixture design \textcolor{red}{ref ((Multiple Hypergeometric Functions: Probabilistic Interpretations and Statistical Uses
James M. Dickey )) ce papier est cite dans LDA pour justifier l'inference approxime, mais je n'arrive pas a trouver ca papier !!!}. However, as reported simulations (figure \ref{fig:gen_burst}), we see that the empirical distributions of the overall degrees are clearly non bursty.

%\input{t/me_generate_burst}


\subsection{Homophily}


\bibliographystyle{unsrt}
\bibliography{./a}

\end{document}
