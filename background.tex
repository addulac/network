\section{Latent Representation}
\label{sec:background}
Without loss of generality, we focus on social networks with binary relationships. A network with $N$ nodes is a graph $G = (V,E)$, where $V$ is a set of nodes (typically representing entities) and $E \in V \times V$ is a set of edges between nodes (typically representing relationships between pairs of entities).The topology of the network is described by the presence or absence of links between nodes in the graph, and can be represented by an adjacency matrix $Y \in \{0,1\}^{N\times N}$.\\

For the link prediction problem in social networks, there is two main approach found in the machine learning literrature. One based on matrix factorization \cite{menon2011link} and the other based on probabilistic models \cite{goldenberg2010survey}. While the former approach often lead to optimization algorithms that scale better, the second offer a richer expressivness and data explanation power, it worth to say that, as for PLSA method, it is always possible to normalize matrices to view the factorization problem as a a probabilistic model. But a deeper relation exists which highlight similarities and differences between both representations. It is due the the assumptions of exchangeability that we make on data, that we expose now.\\

Let's consider the parameters of a network link prediction model consisting of a tuple $\Theta = \{ \mat{F} , \mat{\Phi}\}$ such as $\mat{F} \in \mathcal{F}^{N\times K}$ is a matrix representing the latent features of nodes and $\mat{\Phi} \in \mathcal{W}^{K\times K}$ a matrix of weight interactions such that each feaure vector $(\mat{f}_i)_{i=1}^N$ are i.i.d and each weight $\phi_{kk'}$ for $(k,k') \in K\times K$ are also i.i.d. The Haldous-Hoover theorem \cite{orbanz2015bayesian} tell us that under such assumptions, that likelihood of links in a network are conditionally i.i.d given $\Theta$, and more precisely we have $\p(y_{ij}|\Theta) = \mat{f}_i \Phi \mat{f}_j^T$

The learning problem consist in finding the best model parameters given some criterion to reconstruct the data such as $Y \approx \mat{F}\mat{\Phi}\mat{F}^T$. 

In matrix factorization, we optimize the following criterion which relate to an empirical error minimization estimate:
\begin{displaymath}
    \min\limits_{\Theta} \frac{1}{|Y|} \sum_{(i,j) \in Y} \ell(y_{ij}, \hat y_{ij}(\Theta)) + \mathcal{R}(\Theta)
\end{displaymath}
Where $\ell$ is a loss function and $\cal R$ a regularization term.

Comparatively, in probabilistic modeling, the learning problem is to find the posterior distribution $\p(\Theta | Y)$. The exchangeable assumptions means that the order in which we observe nodes does not matter. It implies that the joint likelihood of data has a factorial distribution such as $\p(Y | \Theta) = \prod_{(i,j) \in Y} \p(y_{ij}| \Theta)$. In this settings, the natural criterion to optimize relate to a Maximum a Posteriori (MAP) estimate: 
\begin{displaymath}
    \max\limits_{\Theta}  \frac{\p(Y | \Theta) \p(\Theta)}{\p(Y)}
\end{displaymath}

Furthermore, using the Jensen inequality and exchangeability assumption, we can show:
\begin{align}
    &- \log(\p(\Theta | Y)) \geq \nonumber \\  
    &\sum_{(i,j) \in Y} \left( \E_{p(\Theta)}[\log(\p(y_{ij}|\Theta))] - \log(\p(y_{ij}|\Theta)) \right) - \log(\p(\Theta))
\end{align}	


This form highlights the equivalence between the matrix and the probabilistic representation \footnote{For completeness we refer the reader to variational inference,that introduce a variational parameters. It shows minimizing the lower bound is equivalent to minimizing the Kulback-Leibler divergence between the true posterior and the variational parameters.}. Moreover, in comparison with th  the matrix factorization objective, the lower bound minimizer has a loss function being the log-evidence and the log-likelihood of observations while the regularization term is equal to the negative of the log prior.


Finally, because we model binary retionship, a natural kernel for the observation is a Bernoullli density such that $y_{ij} \sim \text{Bern}(\Theta)$. It follow from the fact tha the expectation of the bernoulli is it's parameter, we have the following matrix representation for the likelihood:
\begin{equation}
E_{y \sim p(y \mid \Theta)}[Y] = \mat{F} \mat{\Phi}  \mat{F}^T
\end{equation}

This formalism is similar to the work of Buntine on Discrete Component Ananlysis(DCA) \cite{DCA}. In the following we will study two particular models (iMMSB ILFM), that refers to mixed membership model in the sense that the likelihood of the models are defined as a mixture of membership such as:
\begin{equation}
\pr(y_{ij}=1 \mid \Theta ) = \sum_{k, k'} \pr(y_{ij}=1\mid\phi_{k,k'}) \pr(k \mid \mat{f}_i) \pr(k' \mid \mat{f}_j)
\end{equation}

%See the Appendix~\ref{sec:mixmembership} for a justification of eq \eqref{eq:mf} using a Mixed Membership Model approach.
A key point here, is that the Stochatistic Blockmodel well studied in the literrature \cite{goldenberg2010survey}, where nodes of the networks belong only to one clusters (blockmodel) is a special case of this formalism (as well as MMSB and ILFM for extrem case) when the feature vector $f_i$ of node $i$ has one element corresponding to the membership while other are zeros. Besides the choice of prior on $\Phi$ and $\Theta$ represent the context in which  their elements can evolve.

In this framework, we want to answer the following questions:
\begin{itemize}
	\item What properties the model can capture or learn on networks ?
	\item Which constraint on the models can come with an consistent interpretation of latent variables along with the concepts of communities structure and homophily in social networks  ?
\end{itemize} 

%%%

In the next session we review both iMMSB and ILFM more precisely.

