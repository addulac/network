\section{Models}
\emph{Yet another view} ~\\
\label{fig:bayes_net}

Our two models use prior distributions that fall into the two major classes of discrete nonparametric priors. The Hierarchical Dirichlet Process (HDP) that generalizes the Latent Dirichlet Allocation (LDA) for infinite mixtures models. On the other hand, the Indian Buffet Process (IBP), which is the generalization of the Beta-Bernoulli compound distribution (ie Beta Process), which generates infinite binary matrices. The nonparametric models in their truncated version are equivalent to well-known models such as LDA, widely used for text analysis, and Mixed Membership Stochastic Blockmodel which is an adaptation for relational learning.~\\


We adopt the following notation; if a matrix has a negative index superscripted, it indicates that the values corresponding to this index are excluded. A dot $\bm{.}$ in the index means that we marginalize over all possible values.


\subsection{Binary Feature}
In the latent feature models, the features are distributed according to an Indian Buffet Process (IBP), and the weights interaction according to a Gaussian :
\begin{align}
\Theta &\sim \IBP(\alpha)  \quad\text{ is a } N\times K \text{ matrix}\\
\phi_{mn} &\sim N(0, \sigma_w) \quad\text{ for } m,n \in \{1, .., K\}^2
\end{align}
The observation level is defined by deterministically selecting the row of $\Theta$ that encodes the latent features for each nodes that interact. Hence for a node $i$, we note his feature vector by $F_i = \theta_i$ and for $i, j \in V$ we have:

\begin{align}
y_{ij} &\sim \mathrm{Bern}(\sigma(F_i \Phi F_j^\top))
\end{align}
Finally the function $\sigma(x)= \frac{1}{1 + e^{-x}}$ is the sigmoid function to map $[-\infty, +\infty]$ values to [0,1].

\begin{figure}[h]
	\centering
	\minipage{0.25\textwidth}
	\scalebox{0.88}{
	\input{./img/ilfrm2.tex}}
	\endminipage
	\minipage{0.25\textwidth}
	\scalebox{0.88}{
		\input{./img/mmsb2.tex}}
	\endminipage
	\caption{The two Graphical model for (left) the latent feature model and (right) the latent class model. The conceptual difference between the two model are the actualization of latent variables for each interaction. The latent feature model uses the same latent features vector for each node's interaction, while the latent class model draws new variables at each interactions.}
	\label{fig:ilfrm}
\end{figure}

\subsubsection{MCMC Updates for Posterior Inference}

The update for the latent features are obtained by the following Gibbs updates:
\begin{align}
& P(f_{ik} = 1 \mid F^{-ik}) = \frac{m^{-ik}}{N} \\
& P(f_{ik} = 0 \mid F^{-ik}) = 1 - \frac{m^{-ik}}{N}
\end{align}
Where $m^{-ik}$ represents the number of active features $k$ for all entities excluding entity $i$, hence $m^{-ik} = \sum_{j\neq i}f_{jk}$. 

The learning of the weight matrix $W$ is computed using a Metropolis-Hasting algorithm. Thus, we sample sequentially each weight corresponding to non-zeros features interaction.

\begin{equation}
P(\phi_{mn} \mid Y, F, \phi_{-mn}, \sigma_w) \propto P(Y \mid F, \Phi) P(\phi_{mn} \mid \sigma_w)
\end{equation}

We choose a jumping distribution in the same family of our prior on weight centered around the previous sample:

\begin{equation} \label{eq:j_w}
J(\phi_{mn}^* \mid \phi_{mn}) = \mathcal{N}(\phi_{mn}, \eta)
\end{equation}
with $\eta$ a parameter letting us controlling the acceptance ratio.

The acceptance ratio of $\phi_{mn}^*$ is thus:
\begin{equation} \label{eq:r_w}
r_{\phi_{mn}\rightarrow \phi_{mn}^*} = \frac{ P(Y \mid F, \Phi^*)P(\phi_{mn}^* \mid \sigma_w)J(\phi_{mn} \mid \phi_{mn}^*) }{ P(Y \mid F, \Phi)P(\phi_{mn} \mid \sigma_w)J(\phi_{mn}^* \mid \phi_{mn} )}
\end{equation}

\subsubsection{Properties}

In this model, the weight interaction matrix $\Phi$ are not conjugate of the likelihood. Thus it can not be integrated out into a closed form expression. As a matter of simplicity we consider this parameter as known, and omit it the following conditional distribution .

Let $F=\Theta$ and $W=\Phi$, each node $i$ has a fixed feature vector noted $F_i$ and a weighed interactions matrix $W$. In this case, the function $f_i$ is:
\begin{align}
\pr(y_{ij}=1 \mid d_i, F^{-i\bm{.}}) &= \sum_{F_i} \pr(y_{ij}=1 \mid d_i, F^{-i\bm{.}} F_i,) \pr(F_i \mid d_i, F^{-i\bm{.}}) \\
&= \sum_{F_i} \sigma(F_iWF_j^T) \frac{\pr(d_i \mid F^{-i\bm{.}}, F_i) \pr(F_i\mid F^{-i\bm{.}}) }{\pr(d_i\mid F^{-i\bm{.}})}\\
&\propto \sum_{F_i} \prod_{j' \in \V(i) \cup {j}} \sigma(F_iWF_{j'}^T)\prod_{j' \notin \V(i)} 1-\sigma(F_iWF_{j'}^T) \prod_k \frac{m_{ik}}{N}
\end{align}

The last term of equation (7.16) comes from the conditional probability of a feature $f_{ik}$ with an IBP prior and applying a chain of product rule. 

(Reste a valider le passage Ã  la proportion !).It turns out that the $f_i$ is increasing under the assumptions that the model is \emph{strictly} (to define) homophilic. it means that if the similarity between two nodes is greater than one, with a support in $[-1, 1]$, leads to a probability greater than $\frac{1}{2}$. This justify the use of the logistic kernel. 

%The probability that a matrix $F$ is generated from the IBP is ~\cite{IBP}:
%\begin{equation}
%P(F \mid \alpha) = \frac{\alpha^{K_+}}{\prod_{i=1}^N K_1^{(i)} } \exp(-\alpha H_N) \prod_{k=1}^{K_+} \frac{(N - m_k)!(m_k - 1)!}{N!}
%\end{equation}

\subsection{Proportion Feature}

In the latent class model, the rows of the feature matrix $\Theta$ are Dirichlet distributed, and the weights interaction are Beta distributed : 
\begin{align}
\theta_i &\sim \mathrm{Dirichlet}(\alpha_0) \quad\text{ for }  i \in \{1, .., N\} \\
\phi_{mn} &\sim \mathrm{Beta}(\lambda) \quad\text{ for }  m,n \in \{1, .., K\}^2 
\end{align}
The observation level is defined by multinomial draws of class assignments for each nodes. The likelihood for a links depends only on the class assignments of the nodes  and for $i, j \in V$, we have:  
\begin{align}
z_{i\rightarrow j} &\sim \mathrm{Mult}(\theta_i) \\
z_{i\leftarrow j} &\sim \mathrm{Mult}(\theta_j) \\
y_{ij} &\sim \mathrm{Bern}(z_{i\rightarrow j} \Phi z_{i\leftarrow j}^\top)
\end{align}

In this special case, the mapping function $\sigma$ is the identity.

\paragraph{Altenartive Description ! (wich one we keep ?)}~\\

An alternate view, maybe more consistent with the IBP representation is to say that:
\begin{align}
Z &\sim CRF(\alpha_0, \gamma) \\
\phi_{mn} &\sim \mathrm{Beta}(\lambda) \quad\text{ for }  m,n \in \{1, .., K\}^2  \\
y_{ij} &\sim \mathrm{Bern}(\Phi_{z_{i\rightarrow j} , z_{i\leftarrow j}})
\end{align}

In the (not printed here) corresponding graphical model, the arrow between $z_{\rightarrow}$ and $Z$ are dashed as for the IBP representation. Note that $Z \in \{1,.., K\}^{N\times N \times 2}$. ~\\

@ERIC: This representation would justify to explain our interpretation for the Chinese Restaurant Franchise (CRF), because we clearly see it now...

\subsubsection{MCMC Updates for Posterior Inference}~\\

In the latent class model, the collapsed gibbs sampling approach allow us to only sample the classes couple for each observations:

\begin{equation}
\pr(z_{ji}=k, z_{ij}=k' \mid .) \propto \pr(z_{ji}=k \mid .) \pr(z_{ij}=k' \mid .) f_{(k,k')}^{-ji}(y_{ji})
\end{equation}


The class assignment updates for the node couple are:
\begin{align}
\pr(z_{i\rightarrow j} =k \mid Z^{-ij}) &\propto N_{ik}^{-ij} + \alpha_0 \\
\pr(z_{i\leftarrow j} =k \mid Z^{-ij}) &\propto N_{jk}^{-ij} + \alpha_0 
\end{align}

And the likelihood of a link given the couple $c=(k, k')$ is:
\begin{equation} \label{eq:cgs_mmsb}
f_{(k, k')}^{-ij}(y_{ij}=r) = \frac{C_{(k,k')r}^{-ij} + \lambda_r}{C_{(k,k')\bm{.}}^{-ij} + \sum_r\lambda_{r'}} 
\end{equation}

Finally $\Theta$ and $\Phi$ can be reconstructed from the count matrices with the following equations:
\begin{align}
\theta_{ik} &= \frac{N_{ik} + \alpha_0}{ N_{i\bm{.}} + K\alpha_0} \\
\phi_{rc} &= \frac{C_{cr} + \lambda_r}{ C_{c.} + \lambda_{\bm{.}}}
\end{align}

$C$ and $N$ are counts matrices and are describes in section \ref{burst_class}. We refer to the supplementary material for detail of derivations for MCMC updates. 

\subsubsection{Properties}

\label{burst_class}

In the latent class models each  dyads has two underlying  class assignments for each node of the couple. We note $Z \in N\times N\times 2$ the matrix that represents those class assignments.
We seek for the following form of the likelihood, that we marginalize over all the possible couples classes $c=(k, k')$:
\begin{equation} \label{eq:q}
\pr(y_{ij}=1 \mid Y^{-i\bm{.}}, Z^{-ij}, d_i ) = \sum_{c=(k, k')} \pr(y_{ij}=1 \mid Y^{-i\bm{.}}, d_i , c) \pr(c \mid  Z^{-ij} ) 
\end{equation}
Here note that within the sum, the left hand term is conditionally independent of $Z^{-ij}$. And the right hand term is independent of the adjacency terms $Y^{-i\bm{.}}$ since it do not belongs to the Markov blanket of $c$ random variable.

The first term is the likelihood for the links between $(i,j)$ given the class of each node (k, k'). Due to the Beta-Bernoulli conjugacy of the model, $\phi$ and $\theta$ can be marginalized out, and it simplify to: 
\begin{equation} \label{eq:qc}
\pr(y_{ij}=1 \mid Y^{-i\bm{.}}, d_i , c) = \frac{C_{c1}^{-i.} + d_{ic} + \lambda_1}{C_{c\bm{.}}^{-ij} + \lambda_0+\lambda_1} 
\end{equation}
Where $C_{c1}$ denotes the count matrix for all interactions having value 1 (link present) with the classes couple being $c=(k, k')$. Thus $C_{c1} = \sum_{i,j} \bm{1}(z_{i\rightarrow j}=k, z_{i\leftarrow j}=k', y_{ij}=1)$ and $C_{c.} = \sum_{i,j} \bm{1}(z_{i\rightarrow j}=k, z_{i\leftarrow j}=k')$\\

We recognize the likelihood form of the Gibbs update~\cite{HDP}, except that we isolate the term depending of the degree on $i$, $d_i$. Hence the term $d_{ic}$ is the element of the degree with a classes couple $c=(k,k')$ and $d_{ic} = \sum_{j' \neq j} \bm{1}(z_{i\rightarrow j'}=k, z_{i\leftarrow j'}=k', y_{ij'}=1) $.\\

The second term of equation \eqref{eq:q}, can be rewrited by noting that the classes of the couple $c$ are independent and that the term $Y^{-j\bm{.}}$ can be dropped because it is not present in the Markov blanket of the class assignment: 
\begin{equation} \label{eq:topic_assign}
\pr(c \mid  Z^{-ij} ) =  \pr(z_{i\rightarrow j}=k \mid Z^{-ij}) \pr(z_{i\leftarrow j}=k' \mid Z^{-ij})
\end{equation}

Again, the two members of the right hand equation \eqref{eq:topic_assign} are the Gibbs updates for the topic assignments of nodes for the interaction $(i,j)$. Both members reduce to simple form due to the conjugacy between the Dirichlet and Multinomial~\cite{DM} or concurrently from the Chinese Restaurant Franchise~\cite{HDP}:
\begin{align}
\pr(z_{i\rightarrow j}=k \mid Z^{-ij}) = \frac{N_{ik}^{-ij} + \alpha_k}{ N_{i.}^{-ij} + \alpha_{\bm{.}} } \\
\pr(z_{i\leftarrow j}=k' \mid Z^{-ij}) = \frac{N_{jk'}^{-ij} + \alpha_{k'}}{ N_{j\bm{.}}^{-ij} + \alpha_{\bm{.}} } 
\end{align}

Finally, one can see that the only term depending on the degree $d_i$ is isolated, and we can rewrite equation \eqref{eq:q}, with term depending only on $d_i$, $k$, $i$ and $j$:
\begin{equation}
\pr(y_{ij}=1 \mid Y^{-i\bm{.}}, Z^{-ij}, d_i ) = \sum_{c=(k, k')} A_c (B_c + d_{ic})
\end{equation}
Where $A_c$ and $B_c$ are two positive function of $c$.
\begin{align}
A_c &= \frac{N_{ik}^{-ij} + \alpha_k}{ N_{i.}^{-ij} + \alpha_{\bm{.}} } \frac{N_{jk'}^{-ij} + \alpha_{k'}}{ N_{j\bm{.}}^{-ij} + \alpha_{\bm{.}} } \frac{1}{C_{c\bm{.}}^{-ij} + \lambda_0+\lambda_1} \\
B_c &= C_{c1}^{-i.} + \lambda_1
\end{align}

As we sum over all possible couple classes, the probability to have a link will augment with the degree with the classes couple corresponding to the element of the degree with the same couple. Hence the probability to observe a link for node $i$ is strictly crescent with his degree $d_i$. 

\paragraph{Preferential Attachment}~\\

The model is bursty hence it can handle the preferential attachment at the network level.

\paragraph{Local Preferential Attachment}~\\

The Local preferential attachment is similar to the notion of burstiness but inside a community/class of the network. Assuming that we know the class of $i$ $z_{i\rightarrow j}$ to be $k$, the probability to have a link becomes: 
\begin{align}
\pr(y_{ij}=1 \mid Y^{-i\bm{.}}, d_i,z_{i\rightarrow j})  &= \sum_{k'} \pr(y_{ij}=1 \mid Y^{-i\bm{.}}, Z^{-ij}, d_i , c=(k,k')) \pr(z_{i\leftarrow j}=k' \mid Z^{-ij} ) \\
&= \sum_{k'} \frac{C_{c1}^{-i.} + d_{ic} + \lambda_1}{C_{c\bm{.}}^{-ij} + \lambda_0 + \lambda_1} \frac{N_{jk'}^{-ij} + \alpha_{k'}}{ N_{j\bm{.}}^{-ij} + \alpha_{\bm{.}} } \\
&= \sum_{k'} A'_{k'} (B'_{k'} + d_{i(k,k')})
\end{align}

Here the probability increases with the degree independently of the interactions classes. This means that burstiness is possible inside but also between communities.

\paragraph{Communities Distribution}~\\

....Need to count the table for each classes in Chinese Restaurant Franchise (CRF), to evaluate the distribution according to the hyperprior of HDP...


\subsection{Comparison of models}
class proportion vs feature vector \\
Class strength vs feature correlation/metric ? \\
HDP vs IBP \\
complextity $O((E^2K^2)$ vs $O(NK^3)$ \\
property yes/no

