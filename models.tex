\section{Models}
\emph{Yet another view} ~\\
\label{fig:bayes_net}

Our two chosen baseline use prior distributions that fall into the two major classes of discrete nonparametric priors. The Hierarchical Dirichlet Process (HDP) that generalizes the Latent Dirichlet Allocation (LDA) for infinite mixtures models. On the other hand, the Indian Buffet Process (IBP), which is the generalization of the Beta-Bernoulli compound distribution (ie Beta Process), which generates infinite binary matrices. The nonparametric models in their truncated version are equivalent to well-known models such as LDA, widely used for text analysis, and Mixed Membership Stochastic Blockmodel which is an adaptation of the latter for relational learning.~\\


We adopt the following notation; if a matrix has a negative index superscripted, it indicates that the values corresponding to this index are excluded. A dot $\bm{.}$ in the index means that we marginalize over all possible values.


\subsection{Binary Feature}
In the latent feature models, the features are distributed according to an Indian Buffet Process (IBP), and the weights interaction according to a Gaussian :
\begin{align}
\Theta &\sim \IBP(\alpha)  \quad\text{ is a } N\times K \text{ matrix}\\
\phi_{mn} &\sim N(0, \sigma_w) \quad\text{ for } m,n \in \{1, .., K\}^2
\end{align}
The observation level is defined by deterministically selecting the row of $\Theta$ which is distributed according an IBP prior. Hence for a node $i$, we note his feature vector by $F_i = \theta_i$ and for $i, j \in V$ we have:

\begin{align}
y_{ij} &\sim \mathrm{Bern}(\sigma(F_i \Phi F_j^\top))
\end{align}
Finally the function $\sigma(x)= \frac{1}{1 + e^{-x}}$ is the sigmoid function to map $[-\infty, +\infty]$ values to [0,1], a probability space.

\begin{figure}[h]
	\centering
	\minipage{0.25\textwidth}
	\scalebox{0.88}{
	\input{./img/ilfrm2.tex}}
	\endminipage
	\minipage{0.25\textwidth}
	\scalebox{0.88}{
		\input{./img/mmsb2.tex}}
	\endminipage
	\caption{The two Graphical model for (left) the latent feature model and (right) the latent class model. The conceptual difference between the two model are the actualization of latent variables for each interaction. The latent feature model uses the same latent features vector for each node's interaction, while the latent class model draws new variables at each interactions.}
	\label{fig:ilfrm}
\end{figure}

\subsubsection{MCMC Updates for Posterior Inference}

The update for the latent features are obtained by the following Gibbs updates:
\begin{align}
& P(f_{ik} = 1 \mid F^{-ik}) = \frac{m^{-ik}}{N} \\
& P(f_{ik} = 0 \mid F^{-ik}) = 1 - \frac{m^{-ik}}{N}
\end{align}
Where $m^{-ik}$ represents the number of active features $k$ for all entities excluding entity $i$, hence $m^{-ik} = \sum_{j\neq i}f_{jk}$. 

The learning of the weight matrix $W$ is computed using a Metropolis-Hasting algorithm. Thus, we sample sequentially each weight corresponding to non-zeros features interaction.

\begin{equation}
P(\phi_{mn} \mid Y, F, \phi_{-mn}, \sigma_w) \propto P(Y \mid F, \Phi) P(\phi_{mn} \mid \sigma_w)
\end{equation}

We choose a jumping distribution in the same family of our prior on weight centered around the previous sample:

\begin{equation} \label{eq:j_w}
J(\phi_{mn}^* \mid \phi_{mn}) = \mathcal{N}(\phi_{mn}, \eta)
\end{equation}
with $\eta$ a parameter letting us controlling the acceptance ratio.

The acceptance ratio of $\phi_{mn}^*$ is thus:
\begin{equation} \label{eq:r_w}
r_{\phi_{mn}\rightarrow \phi_{mn}^*} = \frac{ P(Y \mid F, \Phi^*)P(\phi_{mn}^* \mid \sigma_w)J(\phi_{mn} \mid \phi_{mn}^*) }{ P(Y \mid F, \Phi)P(\phi_{mn} \mid \sigma_w)J(\phi_{mn}^* \mid \phi_{mn} )}
\end{equation}



\subsection{Proportion Feature}

In the latent class model, the rows of the feature matrix $\Theta$ are Dirichlet distributed, and the weights interaction are Beta distributed : 
\begin{align}
\theta_i &\sim \mathrm{Dirichlet}(\alpha_0) \quad\text{ for }  i \in \{1, .., N\} \\
\phi_{mn} &\sim \mathrm{Beta}(\lambda) \quad\text{ for }  m,n \in \{1, .., K\}^2 
\end{align}
The observation level is defined by multinomial draws of class assignments for each nodes. The likelihood for a links depends only on the class assignments of the nodes  and for $i, j \in V$, we have:  
\begin{align}
z_{i\rightarrow j} &\sim \mathrm{Mult}(\theta_i) \\
z_{i\leftarrow j} &\sim \mathrm{Mult}(\theta_j) \\
y_{ij} &\sim \mathrm{Bern}(z_{i\rightarrow j} \Phi z_{i\leftarrow j}^\top)
\end{align}

In this special case, the mapping function $\sigma$ is the identity.

\paragraph{Altenartive Description ! (wich one we keep ?)}~\\

An alternate view, maybe more consistent with the IBP representation is to say that:
\begin{align}
Z &\sim CRF(\alpha_0, \gamma) \\
\phi_{mn} &\sim \mathrm{Beta}(\lambda) \quad\text{ for }  m,n \in \{1, .., K\}^2  \\
y_{ij} &\sim \mathrm{Bern}(\Phi_{z_{i\rightarrow j} , z_{i\leftarrow j}})
\end{align}

In the (not printed here) corresponding graphical model, the arrow between $z_{\rightarrow}$ and $Z$ are dashed as for the IBP representation. Note that $Z \in \{1,.., K\}^{N\times N \times 2}$. ~\\

@ERIC: This representation would justify to explain our interpretation for the Chinese Restaurant Franchise (CRF), because we see it explicitly here...

\subsubsection{MCMC Updates for Posterior Inference}~\\

In the latent class model, the collapsed gibbs sampling approach allow us to only sample the classes couple for each observations:

\begin{equation}
\pr(z_{ji}=k, z_{ij}=k' \mid .) \propto \pr(z_{ji}=k \mid .) \pr(z_{ij}=k' \mid .) f_{(k,k')}^{-ji}(y_{ji})
\end{equation}


The class assignment updates for the node couple are:
\begin{align}
\pr(z_{i\rightarrow j} =k \mid Z^{-ij}) &\propto N_{ik}^{-ij} + \alpha_0 \\
\pr(z_{i\leftarrow j} =k \mid Z^{-ij}) &\propto N_{jk}^{-ij} + \alpha_0 
\end{align}

And the likelihood of a link given the couple $c=(k, k')$ is:
\begin{equation} \label{eq:cgs_mmsb}
f_{(k, k')}^{-ij}(y_{ij}=r) = \frac{C_{(k,k')r}^{-ij} + \lambda_r}{C_{(k,k')\bm{.}}^{-ij} + \sum_r\lambda_{r'}} 
\end{equation}

Finally $\Theta$ and $\Phi$ can be reconstructed from the count matrices with the following equations:
\begin{align}
\theta_{ik} &= \frac{N_{ik} + \alpha_0}{ N_{i\bm{.}} + K\alpha_0} \\
\phi_{rc} &= \frac{C_{cr} + \lambda_r}{ C_{c.} + \lambda_{\bm{.}}}
\end{align}

$C$ and $N$ are counts matrices and are describes in section \ref{burst_class}. We refer to the supplementary material for detail of derivations for MCMC updates. 


\subsection{Comparison of models}
class proportion vs feature vector \\
Class strength vs feature correlation/metric ? \\
HDP vs IBP \\
complextity $O((E^2K^2)$ vs $O(NK^3)$ \\
property yes/no

