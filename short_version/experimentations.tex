\section{Illustration}
%\section{Experimentations}

our experimental platform that we have used for our experiments which is available online. Notes that it makes our experiments easily fully reproducible \footnote{https://github.com/dtrckd/pymake}.

To illustrate our theoretical results, we evaluate the predictive performance and the ability of the models to capture homophily and preferential attachment on artificial and real networks. For homophily, we simply compare the distributions of the natural and latent similarities on linked and non-linked pairs of nodes. For global preferential attachment, we plot (as done in \cite{???}) the degree distribution and its corresponding best fitted line in the log-log scale. In addition, we use the measure developed in \cite{clauset2009power} for assessing whether empirical data behaves according to a power law (as mentioned before, power laws are the standard bursty distributions in social networks \cite{barabasi1999emergence}). This framework combines maximum-likelihood methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic to compute a $p$-value. If the obtained $p$-value is large (close to 1), then the data is likely to be distributed according to a power law and the associated network displays preferential attachment;  on the other hand, if it is small, the data is likely not distributed according to a power law and the associated network does not display preferential attachment.

For local preferential attachment, we follow the same approach as before to compute the $p$-value, the only difference being that the empirical data does not correspond any longer to the global adjacency matrix, but to reduced matrices for each class. The computation of the reduced adjacency matrices varies from one model to the other:
%
\begin{itemize}
    \item For IMMSB, for a given class $k$, the reduced adjacency matrix $Y^k$ is defined by: $y_{ij}^k=1$ if $y_{ij}=1, z_{i\rightarrow j}=z_{i\leftarrow j}=k$ and $0$ otherwise.
        \item For ILFM, the reduced adjacency matrix $Y^k$ is defined by:$ y_{ij}^k=1$ if $y_{ij}=1 , f_{ik}=f_{jk}=1$ and $0$ otherwise.
\end{itemize}
%

\subsection{Datasets and model parameters}

To illustrate the above developments, we consider two artificial and two real networks, the characteristics of which are summarized in Table~\ref{table:networks_measures}.

\begin{table}[h] 
	\centering
	\caption{Characteristics of artificial and real networks.}
	%\resizebox{\textwidth}{!}{  
    \begin{tabular}{lrrr}
        \hline
        \textbf{Networks} &   nodes &   edges &   density \\
        \hline
        Network1 &    1000 &    3507 &     0.007 \\
        Network2 &    1000 &   31000 &     0.062 \\
        Blogs         &    1490 &   20512 &     0.009 \\
        Manufacturing &     167 &    5950 &     0.215 \\
    \hline
    \end{tabular}
	\label{table:networks_measures}
\end{table}

The non-oriented artificial networks (Network1 and Network2) have been generated with the DANCer-Generator \cite{largeron2015}. This generator has been chosen because it allows one to build an attributed graph having a community structure as well as known properties of real-world networks such as preferential attachment and homophily. Moreover, by modifying the parameters, these properties can be weakened. Netwrok1 was generated to be \textcolor{orange}{???} whereas Network2 was generated to be \textcolor{orange}{???}.

The first real network, denoted Blogs \footnote{available at: http://moreno.ss.uci.edu/data.html\#blogs}, contains front-page hyperlinks between blogs in the context of the 2004 US election. A node represents a blog and an edge represents a hyperlink between two blogs. The second one, denoted Manufacturing \footnote{available at: https://www.ii.pwr.edu.pl/~michalski/index.php?content=datasets\#manufacturing}, is an internal email communication network between employees of a mid-sized manufacturing company. Each node is associated to an employee and an oriented link represents an email sent between the two employees. One can notice that the second network is specific since it is an enterprise network in which the relationships between the employees are (professionally) constrained. This means that this network is less likely to display some of the properties that occur in unconstrained social networks.

The adjacency matrices and global degree distributions of these networks are presented in Figure \ref{fig:corpuses}. The adjacency matrices enable us to visualize some characteristics of the networks such as their density and their clustering patterns: as one can note, Blogs and the two artificial networks (Network1 and Network2) have a clear community structure, corresponding to the blocks of white dots on the figure, whereas Manufacturing, the denser network, does not have such a structure. Furthermore, the log-log scale plots show that Network1 and Blogs verify the  global preferential attachment (the fitted line represents relatively well the data points) whereas neither Network2 nor Manufacturing verify it. This is confirmed by the $p$-values reported in the first section of Table \ref{table:me_gofit} (Training Datasets): the $p$-value is 1 for Network1 and Blogs, whereas it is null for Network2 and Manufacturing. Furthermore, the parameter $\alpha$ in the global case (right) \textcolor{orange}{???}

Figure \ref{fig:synt_graph_local} represents the local degree distributions for all networks, each curve in each plot being associated to a different class. As the ground truth is not available for the real networks (Blogs and Manufacturing), classes have been determined with the Louvain algorithm \cite{Blondel2008} and the local distribution defined according to the obtained classes. As one can note, the plots for Network1 and Blogs are linear for the most important degrees, whereas the plots for Network2 and Manufacturing do not display any clear pattern, suggesting that Network1 and Blogs satisfy, at least partly, local preferential attachment whereas Network2 and Manufacturing do not. This is confirmed by the $p$-values reported in Table~\ref{table:me_gofit}: the $p$-vlaue equals to $1$ for Network1 and BLogs, and $0$ and $0.4$ for Network2 and Manufacturing.

Thus, for the preferential attachment, global and local, we can distinguish Network1 and Blogs which satisfy the preferential attachment and Network2 and Manufacturing which do not exhibit the property.

\begin{figure}[h]
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/network1_dd}
        \end{minipage}
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/network2_dd}
        \end{minipage}
        %\vskip\baselineskip
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/blogs_dd}
        \end{minipage}
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/manufacturing_dd}
        \end{minipage}
	\caption{Adjacency matrices (left) and global degree distributions (right) for the four training datasets. In the adjacency matrices, a white dot corresponds to a 1 and a block dot to a 0.}
	\label{fig:corpuses}
\end{figure}

\begin{figure}[h]
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/network1_1}
        \end{minipage}
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/network2_1}
        \end{minipage}
        \vskip\baselineskip
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/blogs_1}
        \end{minipage}
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{img/corpus/manufacturing_1}
        \end{minipage}
        \caption {Local degree distributions for the four training datasets. For Network1 and Network2 the classes come from ground-truth. For Blogs and Manufacturing we use classes found by the Louvain algorithm.} 
	\label{fig:synt_graph_local}
\end{figure}

For each dataset described earlier, we run a MCMC inference consisting of 200 iterations to learn the posterior distribution for the IMMSB and ILFM  models described previously. For IMMSB, the concentration parameters of HDP were optimized  using vague gamma priors $\alpha_0 \sim \text{Gamma}(1,1)$ and $\gamma \sim \text{Gamma}(1,1)$ following \cite{HDP}. The parameters for the matrix weights  $\lambda_0$ and $\lambda_1$ were fixed to 0.1. For ILFM, the weights hyperparameter  $\sigma_w$ was fixed to 1 and the IBP hyperparameter $\alpha$ to 0.5 in order to  have comparable number of classes with IMMSB. This inference procedure was run ten times and the average values are reported as final results.

\subsection{Homophily}

Figure \ref{fig:homo_mustach} presents boxplots describing the distributions of the natural ($s_n(i,j)$) and latent ($s_l(i,j)$) similarities computed respectively on linked and non-linked pairs of nodes for IMMSB (left) and ILFM (right). The results have been aggregated over the four datasets. They confirm that the natural similarity is  higher for pairs of nodes which are linked than for pairs of nodes which are not linked, for both models. 
%However, one can notice that, for IMMSB, the similarities computed on the non linked pairs are more concentrated around zero which provides an interesting insight of the bursty behavior of this model.
For the latent similarity,  there is no difference between the linked and non-linked pairs, indicating that the links are not homophilic. These experimental results are in line with the theoretical results presented in Section~\ref{sec:homophily} that state that both ILFM and IMMSB are homophilic with respect to the natural similarity but are not homophilic for the latent similarity.

\begin{figure}[h]
\centering
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/homo_mustach_immsb}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/homo_mustach_ilfm}
    \end{minipage}
    \caption{Natural and latent similarities aggregated over all datasets and computed on linked and non-linked pairs of nodes for IMMSB (left) and ILFM (right).}
    \label{fig:homo_mustach}
\end{figure}


\subsection{Preferential attachment}

Table \ref{table:me_gofit} reports the value of the power-law goodness of fit for IMMSB and ILFM in the global case (left) and in the local case (right). It appears that for both models, the global preferential attachment is only verified for networks generated from datasets where the property was observed, namely in Network1 with p-value equals to 0.9 for IMMSB and 1 for ILFM, and in Blogs with a p-value equals to 1 for the both models whereas the property is not verified in Network2 and in Manufacturing, where p-values equal 0. This is in accordance with Proposition 2.1 according to which, both ILFM and IMMSB do not satisfy global preferential attachment. However, these models are able to capture this property if it exists in the training dataset.  Moreover, we can observe that, in the local case, IMMSB complies with the preferential attachment with $p$-values equals or close to 1 for the four networks while, ILFM obtained low p-values for the networks that were less locally bursty respectively equal to 0 and 0.3 for Network2 and Manufacturing. Also, the power-law coefficients $\alpha$ are significantly greater for IMMSB than for ILFM and specially for the bursty networks Network1 and Blogs.

Figure \ref{fig:me_local} illustrates the local preferential attachment by plotting the local degree distributions for Network1 (top) and Network2 (bottom) learned with IMMSB (left) and ILFM (right). The shape of local degree distributions appears more linear for IMMSB and with more fluctuations for ILFM. This illustrates the inability  of ILFM to capture local preferential attachment property,  as stated in Proposition 2.2. 

\begin{table}[t]
\caption{Preferential attachment measures for training datasets and networks generated with fitted models.}
\centering
\begin{tabular}{lrrrr}
  \multirow{2}{*}{\textbf{Training Datasets}}  &
  \multicolumn{2}{c}{Global} & \multicolumn{2}{c}{Local}\\
  \cmidrule(r){2-3} \cmidrule(l){4-5}
  &   $p$-value &   $\alpha$   & $p$-value & $\alpha$   \\
\hline
Network1       & 1 & 2.4 &   1.0 $\pm$ 0.0  &  1.8 $\pm$ 0.03  \\
Network2       & 0 & 1.3 &   0.0 $\pm$ 0.0  &  1.2 $\pm$ 0.01 \\
Blogs          & 1 & 1.5 &   1.0 $\pm$ 0.0  &  1.4 $\pm$ 0.03\\
Manufacturing  & 0 & 1.4 &   0.4 $\pm$ 0.3  &  1.3 $\pm$ 0.05 \\
\hline

  \ \textbf{IMMSB} &&&& \\
\hline
Network1       & 0.9 & 1.4 &   1.0 \(\pm\) 0.0   &  3.5 \(\pm\) 0.7 \\
Network2       & 0 & 1.3 &   0.9 \(\pm\) 0.0   &  1.6 \(\pm\) 0.2 \\
Blogs          & 1 & 1.3 &   1.0 \(\pm\) 0.0   &  4.3 \(\pm\) 1.1 \\
Manufacturing  & 0 & 1.2 &   0.9 \(\pm\) 0.01  &  1.6 \(\pm\) 0.1 \\
\hline

  \ \textbf{ILFM} &&&& \\
\hline
Network1      & 1 & 1.4 &   1.0 \(\pm\) 0.0  &  1.7 \(\pm\) 0.1 \\
Network2      & 0 & 1.2 &   0.0 \(\pm\) 0.0 &  1.2 \(\pm\) 0.0 \\
Blogs         & 1 & 1.3 &   0.9 \(\pm\) 0.2  &  1.5 \(\pm\) 0.1 \\
Manufacturing & 0 & 1.2 &   0.3 \(\pm\) 0.3  &  1.3 \(\pm\) 0.0 \\
\hline
\end{tabular}
\label{table:me_gofit}
\end{table}

\begin{figure}[h]
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/immsb_network1_1}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/ilfm_network1_1}
    \end{minipage}
    \vskip\baselineskip
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/immsb_network2_1}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/ilfm_network2_1}
    \end{minipage}
    \caption {Local degree distributions for Network1 (top row) and Network2 (bottom row) generated with fitted models IMMSB (first column) and ILFM (second column).} 
\label{fig:me_local}
\end{figure}

\begin{figure}[h]
\centering
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/roc_network1_20_f}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/roc_network2_20_f}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/testset_max_20.png}
    \end{minipage}
    \caption{Top: AUC-ROC curves for Network1 (left) and Network2(right) with 20 percent of data used as learning set. Bottom: Relative performance of IMMSB and ILFM in function of percentage of data used as testing set.} 
\label{fig:auc}
\end{figure}


%\paragraph{Prediction performance.}

Figure \ref{fig:auc} compares the performance  of the models on the different datasets in function of the training set size. In the bottom plot,  y-axis gives the relative performance defined as the difference of the AUC values obtained for IMMSB and ILFM: $AUC_{IMMSB} - AUC_{ILFM}$ whereas x-axis indicates the percentage of data (links) randomly removed from the datasets and  used as a testing set. Hence, the number of training data decreases with the x-axis and a positive value on the y-axis indicates that IMMSB outperforms ILFM.  Note that the relative performance corresponds to the difference of the MAX AUC values obtained for both models on the ten inference experiences.

The top plot illustrates a case where 20 percent of the data is used as testing set and where IMMSB dominates ILFM on Network1 (left) and the opposite for Network2 (right).

In general as shown in bottom plot, ILFM obtains better performance than IMMSB. However, the relative predictive performance of IMMSB  increases  when the quantity of training data decreases whereas for non-bursty networks the results are the opposite: the performance of ILFM increases when the size of the learning dataset decreases. This is particularly visible for Network2, more contrasted for Manufacturing, probably due to the small size of this last network which makes the prediction less challenging.


