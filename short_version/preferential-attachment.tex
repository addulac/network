\section{Preferential attachment: \emph{"The rich get richer"}}
\label{sec:burstiness}

Preferential attachment, sometimes referred to as the \textit{rich get richer} rule, is a mechanism according to which each node is connected to an existing node with a probability that increases with the number of links of the chosen node\footnote{This property is well captured by a power law distribution, hence the claim often made that preferential attachment translates as a power law distribution for the node degrees.}. However, as noted in Leskovec \textit{et al.}, usually, in social networks, entities do not have a global knowledge of the network. The preferential attachment model is thus more likely to be local, and to be specific to communities \cite{LeskovecBKT08}.

Preferential attachment relates to a general phenomenon known as \textit{burstiness}\footnote{A.L. Barab\'asi, for example, uses the term \textit{preferential attachement} in \cite{barabasi1999emergence}, and \textit{burstiness} in \cite{barabasi_burst}.} which describes the fact that some events appear in bursts, \textit{i.e.} once they appear, they are more likely to appear again. Burstiness has been studied in different fields, in particular in computational linguistics and information retrieval to characterize word occurrences \cite{church1995poisson}. In these domains, simple definitions of burstiness, that directly capture the fact that a probability distribution is bursty if the probability of generating a new occurrence of an event increases with the number of occurrences of this event, have been proposed \cite{clinchant2008bnb,clinchant2010information}, for both discrete and continuous probability distributions. We adapt here these definitions for preferential attachment in social networks.

In the context of social networks, the notion of preferential attachement amounts to the fact that the more links a node has (\textit{i.e.} the higher its degree), the more likely it will be linked to new nodes. As mentioned before, this phenomenon however appears at different levels: globally for the whole network, and locally within classes. For global preferential attachment, the degree of a node is a known integer; for local preferential attachment, in most models, the exact degree is not known, and one has to rely on an estimate of it, as done in the following definition:
%
\begin{definition}[Preferential attachment]
Let $i$ be a node in a social network and let $d_i$ denote its degree. 
\begin{description}
 \item[(1)] \emph{Global Preferential Attachment}: we say that a probabilistic link prediction model $\mathcal{M}_e$ satisfies the global preferential attachement iff, for any node $i$, $\pr(d_i \ge n+1 \mid d_i \ge n, \mathcal{M}_e)$ increases with $n \in \mathbb{N}$;
 \item[(2)] \emph{Local Preferential Attachment}: we say that a probabilistic link prediction model $\mathcal{M}_e$ satisfies the local preferential attachement iff, for any node $i$, denoting $d_{i,k}$ the degree of node $i$ in class $k$, $\forall \epsilon \in [0,1], \, \pr(d_{i,k} \ge x+\epsilon \mid d_{i,k} \ge x, \mathcal{M}_e)$ increases with $x \in [0,N-1]$. Furthermore, $d_{i,k}$ is defined as the expectation, over all nodes in the network, of forming a link through latent factor $k$.
\end{description}
\label{def:burst-soc-net}
\end{definition}
%
As one can note, these definitions directly translate the fact that "the more connections a node has, the more likely it is to be connected to new nodes". The only difference between the local and global cases lies in the fact the degree is usually unknown in the local case, and is here estimated through its expectation.

For global preferential attachment, the degree $d_i$ directly corresponds to the number of outgoing links of node $i$. Exploiting the fact that the observations are independent given $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$, one has:
%
\begin{align}
\pr(d_{i} \ge n+1 \mid d_{i} \ge n, \mathcal{M}_e) = 1 - \prod_{j \notin \mathcal{V}(i)} P(y_{ij} = 0 \mid d_{i} \ge n, \mathcal{M}_e) \nonumber \\
= 1 - \prod_{j \notin \mathcal{V}(i)} (1 - P(y_{ij} = 1 \mid d_{i} \ge n, \mathcal{M}_e)) \nonumber
\end{align}
%
where $\mathcal{V}(i)$ denotes the set of nodes connected to node $i$. Hence $\pr(d_{i} \ge n+1 \mid d_{i} \ge n, \mathcal{M}_e)$ and $P(y_{ij} = 1 \mid d_{i} \ge n, \mathcal{M}_e)$ vary in the same direction. 

For \ifm, one has: $\pr(y_{ij}=1 | d_i \ge n, \mathcal{M}_e) = \sigma(\mat{\hat{f}}_i \mat{\hat{\Phi}} \mat{\hat{f}}_j^\top)$, which is independent of $n$. Similarly, for \imb, one has, by marginalizing over the $z$ variables: $\pr(y_{ij}=1 | d_i \ge n, \mathcal{M}_e) = \mat{\hat{f}}_i \mat{\hat{\Phi}} \mat{\hat{f}}_j^\top$, which is also independent of $n$. We thus have the following property:
%
\begin{proposition}[]
\label{pref-attch-glob}
Both \ifm\ and \imb\ do not satisfy global preferential attachment.
\end{proposition}

For local preferential attachment, the situation is slightly more complex:
%
\begin{proposition}[]
\label{pref-attch-loc}
\imb\ satisfies local preferential attachment whereas \ifm\ does not.
\end{proposition}
%
\noindent \textbf{Proof} Let $y_{ij,k}$ be the binary random variable that is $1$ if nodes $i$ and $j$ are linked through the latent factor $k$ and $0$ otherwise. Then, $d_{i,k} = \sum_{j \in \mathcal{V}(i)} \pr(y_{ij,k} =1 | \mathcal{M}_e)$. For \imb, this leads to $d_{i,k} = \sum_{j \in \mathcal{V}(i)} \hat{f}_{ik} \hat{\Phi}_{kk} \hat{f}_{jk} = \hat{f}_{ik} \sum_{j \in \mathcal{V}(i)} \hat{\Phi}_{kk} \hat{f}_{jk}$. The positive reinforcement effect of the Dirichlet Process \cite{teh2006hierarchical} at the basis of \imb\ corresponds to a burstiness phenomenon and directly translates, for any $i$ and any $k$, as: $\pr(\hat{f}_{ik} \ge x'+\epsilon' \mid \hat{f}_{ik} \ge x',\mathcal{M}_e)$ increases with $x'$ (for all $\epsilon'$ and $x'$ chosen in accordance with the domain of definition of $\hat{f}_{ik}$). Setting $x=x'(\sum_{j \in \mathcal{V}(i)} \hat{\Phi}_{kk} \hat{f}_{jk})$ and $\epsilon = \epsilon'(\sum_{j \in \mathcal{V}(i)} \hat{\Phi}_{kk} \hat{f}_{jk})$ and exploiting the fact that $\sum_{j \in \mathcal{V}(i)} \hat{\Phi}_{kk} \hat{f}_{jk}$ is positive and independent of $i$ leads to: $\pr(d_{i,k} \ge x+\epsilon \mid d_{i,k} \ge x, \mathcal{M}_e)$ increases with $x$, which proves that \imb\ satisfies the local preferential attachment effect. For \ifm, as $\hat{f}_{ik}$ is binary, there no positive reinforcement effect associated to it and the above reasoning does not hold (all the more so that $\hat{\Phi}_{kk}$ can be negative). As the factor matrix is binary, one has:
%
\[ 
d_{i,k} = \sum_{j \in \mathcal{V}_i} \sigma(\hat{f}_{ik} \hat{\Phi}_{kk} \hat{f}_{jk}) =  \sigma(\hat{\Phi}_{kk}) |\{j \in \mathcal{V}(i), \hat{f}_{jk} =1\}|
\]
%
where $|.|$ denotes the cardinal of a set. The local degree for \ifm\ thus amounts to the number of nodes connected to $i$ that belong to latent factor $k$, weighted by $\sigma(\hat{\Phi}_{kk})$. As $\sigma(\hat{\Phi}_{kk})$ is strictly positive,  $\pr(d_{i,k} \ge x+\epsilon \mid d_{i,k} \ge x, \mathcal{M}_e) = \pr(|\{j \in \mathcal{V}(i), \hat{f}_{jk} =1\}| \ge (x+\epsilon)/ \sigma(\hat{\Phi}_{kk}) \mid |\{j \in \mathcal{V}(i), \hat{f}_{jk} =1\}| \ge x/\sigma(\hat{\Phi}_{kk}), \mathcal{M}_e)$. The reasoning for global preferential attachement thus applies here, showing that \ifm\ does not satisfy the local preferential attachment effect. \hspace{7.1cm} $\Box$

The above propositions show that both models are deficient in the sense that they do not guarantee that the networks they generate will comply to the global (and local in case of \ifm) preferential attachment phenomena, which are inherent properties of the probability distributions underlying the models. This does not mean however that \ifm\ and \imb\ are not able to model well social networks during the learning phase, even according to the underlying degree distribution. Indeed, the Gibbs updates for both models will assign higher weight to nodes and factors that have been used during the learning phase. Provided there is enough training data, both models will likely reproduce the degree distributions observed in the training data. We will observe that in the following section, devoted to the illustration of the properties we have established.
