
\subsection{Preferential Attachments: Extensions and generalization}

In this section we provide an extension of the study of the preferential attachment in two directions :
\begin{itemize}
    \item Recall the representation theorem \cite{orbanz2015bayesian} about exchangeable sequence and arrays and it's consequences on random networks.%Show the relation between exchangeable sequence (jointly exchangeable adjacency matrix) and the burstiness phenomenon associated to the sum over realization of the exchangeable sequence,
    \item Characterizing the burstiness phenomenon over the size of the classes that the models produce in their generative process (feature burstiness). 
    \item Specifying the limitation of studying the preferential attachment in the $me$ context by specifying it's relation with $mg$. The $mg$ context represents the models seen from it's hyper-parameters while $me = (\hat F, \hat \Phi)$ represents the model seen from an estimated model (with a inference procedure like a MCMC or a variational inference).
\end{itemize}

\subsubsection{Recall of Definitions and Theorems on Exchangeability}~\\

An exchangeable sequence is a sequence $X := (X_1 \in A_1, X_2 \in A_2,...)$ of random variables whose joint distribution satisfies :
\begin{equation}\label{eq:exch}
P(X_1 \in A_1, X_2 \in A_2,...) = P(X_{\pi(1)} \in A_1, X_{\pi(2)} \in A_2,...)
\end{equation}

for every permutation $\pi$  of integers and collection $A_1, A_2,...$ of measurable sets. Following the notation in \cite{orbanz2015bayesian} we can express \eqref{eq:exch} by $P(X_n) = P(X_{\pi(n)})$. In order to characterize the adjacency matrix underlying a social network, we can extend the definition of sequence by denoting a random matrix as $(X_{ij})_{i,j \in \mathbb{N}}$. Thus we says that the random matrix $X$ is jointly exchangeable if it satisfies :
	\begin{equation}
	P(X_{ij}) = P(X_{\pi(i)\pi(j)})
	\end{equation}
	
The Aldous-Hoover theorem generalizes the representation theorem for exchangeable sequence of De-Finetti for exchangeable matrix. It states that a random matrix $(X_{ij})$ is a jointly exchangeable matrix if and only if it can be represented as follows : There is a random measurable function $F:[0,1]^3 \rightarrow X$ such that : 
\begin{equation}
X_{ij} \sim F(U_i, U_j, U_{\{\{i,j\}\}})
\end{equation}

where $(U_i)_{i\in \mathbb(N)}$ and $(U_{\{\{i,j\}\}})_{i, j\in \mathbb(N)}$ are, respectively, a sequence and matrix of i.i.d. Uniform[0,1] random variables.

A important consequence of the Aldous-Hoover theorem is that an exchangeable graph is either empty or dense. It means that if the graph is not empty, the number of edges $n$ will be lower bounded by $cn^2$ with $c$ a constant. This  come from the the fact that edge proportion is independent of $N$ and can be computed as follows $\epsilon = \int_{[0,1]^3} F(x, y, z)dxdydz$.

~\\
We illustrate this results for (i)MMSB model (for the nonparametric version, note that the support of $k$, and $\bm\alpha$ is theoretically infinite and in the latter drawn from a stick breaking process  $\bm \alpha \sim SB(\gamma)$), by marginalizing over the model parameters, we can compute the expected proportion of edges in the graph $G$ :

\begin{align*}
&p(y_{ij}=1| \bm{\alpha}, \bm{\lambda}) = \int_{f_i} \int_{f_j} \int_{\Phi} p(f_i| \alpha ) p(f_j| \alpha)p(\Phi| \lambda) \\
&\times \sum_{k,k'} p(y_{ij} \mid \phi_{kk'})\ p(k\mid f_i)p(k'\mid f_j)df_i df_j d\Phi \\
&=  \sum_{k,k'} \int_{\Phi} \phi_{kk'} p(\Phi| \lambda) d\Phi \int_{f_i} f_{ik} p(f_i| \alpha )df_i \int_{f_j} f_{jk}  p(f_j| \alpha ) df_j \\
&= \sum_{k,k'} \E[B(\lambda_1, \lambda_0)] \E_k[Dir(\mat{\alpha} )] \E_{k'}[Dir(\mat{\alpha})] \\
&= \frac{\lambda_1}{\lambda_0+\lambda_1}\frac{\sum_{k,k'} \alpha_k\alpha_k'}{(\sum_k \alpha_k)^2} = \epsilon
\end{align*}

It follows that this expectation do not depend on the number of vertex $n$, and the expected number of edges for an undirected graph is $\dbinom{n}{2} \epsilon = O(n^2)$.

Note that for ILFM, the expected proportion of edges in the graph don't take a closed form expression since the (kernel) likelihood is not conjugate with the Indian Buffet Process :

\begin{equation}
P(y_{ij}=1 | \alpha, \sigma) = \int_\Phi \sum_F p(y_{ij}=1 | \Phi, F)P(F | \alpha) P(\Phi|\sigma) d\Phi
\end{equation}

There is no closed form for this expression due to it's non-conjugacy.

Here is the different component of this previous equation for completeness \cite{IBP} :
\begin{equation}
p(y_{ij}=1 | \Phi, F) = \sigma(f_i\Phi f_j^T)
\end{equation}

\begin{equation}
p(\Phi|\sigma) = \prod_{k, k'} P(\phi_{k,k'}|\sigma) \qquad \mathrm{with} \quad P(\phi_{k,k'}|\sigma) = N(0, \sigma)
\end{equation}

\begin{equation}
P(F \mid \alpha) = \frac{\alpha^{K_+}}{\prod_{i=1}^N K_1^{(i)} } \exp(-\alpha H_N) \prod_{k=1}^{K_+} \frac{(N - m_k)!(m_k - 1)!}{N!}
\end{equation}

Where $H_N = \sum_{j=1}^N \frac{1}{j}$ is the Nth harmonic number, $m_k$ the number of observations containing feature $k$ and $K_+=K-K_0$ is the number of represented features where $K_0$ is the number of observation for which $m_k=0$.



\subsubsection{Feature burstiness}~\\

We propose a third definition for the study of the preferential attachment in social networks. It concerns the preference that an individuals would have to join a class given the current number of actual members. Thus the quantity we study here correspond to the size of the classes : thus we add the following third definition to the previous section : 

To pursue this definition, one need to precise what precisely means the "size of a class". Depending on what is the definition of the membership within a class that we consider, the notion of size will differs.

We consider the two following cases, which defined the relative size $N_k^i$ for any node $i$. It represents the size of class from a node point a view and can be interpreted as the potential number of nodes that a given node $i$ can bind to :
\begin{itemize}
    \item Soft membership - IMMSB : In this case, nodes belongs to every classes in different proportion. The size of class refers in this case to the number of relation (linked or non-linked) within a class who depends on the random tensor $Z$ : $N_k^i = \sum_{j \in V} \delta(z_{i\rightarrow j}=k)\delta(z_{j\rightarrow i}=k)$,
    \item Hard membership - ILFM : This case correspond to the number of nodes falling joitly in a class since the membership is in the binary set \{ \emph{belongs} or \emph{not belongs} \}, which depends on the random matrix $F$ : $N_k^i = \sum_{j\in V} f_{ik}f_{jk}$.
\end{itemize}

From that definition, one can compute the size of a class as $N_k = \sum_{i\in V} N_k^i$ and the following upper bounds hold for an undirected network model with self loop: $N_k^i \leq N$ and $N_k \leq \dbinom{N}{2}$.

The question that arise is how are distributed the $N_{1:k}$ random variables and what is their bursty dynamic.

\begin{definition}[Preferential attachment (for class size)]
Let $k$ be a class in a social network and let $N_k$ denote its size. 
\begin{description}
    \item[(3)] \emph{feature preferential attachment}: we say that a probabilistic link prediction model $mg$ satisfies the feature preferential attachment iff, for any latent class $k$, the model satisfy the following , $\pr(N_k \ge n+1 \mid N_k \ge n, \mathcal{M}_g)$ increases with $n \in [0,\dbinom{N}{2}]$.
\end{description}
\label{def:burst-soc-net2}
\end{definition}


\begin{proposition}
Both ILFM and IMMSB respect the feature preferential attachment (with regularities conditions on hyperparameters for IMMSB).	
\end{proposition}

\begin{proof}[sketch]
	
For ILFM:

From node exchangeability on can write :
\begin{equation} \label{eq:feat_ex}
P(N_k=n) = \dbinom{N}{n}P(f_{1k}=1,..,f_{nk}=1,f_{(n+1)k}=0,..,f_{Nk}=0)
\end{equation}

It follows from the burstiness definition and Bayes' rule :
\begin{equation}
\frac{P(N_k=n+1)}{P(N_k=n)} = \frac{N-n}{n+1}P(f_{ik}=1 | F_{.k}^{-ik})
\end{equation} 

Here, the node index $i$ is a free variable since the sequence in equation \eqref{eq:feat_ex} is exchangeable, who can choose any nodes.

As $F$ is drawn from an Indian Buffet Process, we have $P(f_{ik}=1 | F_{.k}^{-i}) = \frac{n}{N}$.\\


For IMMSB: 

Remark: note if $N_k^i$ is bursty, it implies that $N_k$ is also busrty from its definition because the sum conserve the inequality. And in the former case the burstiness is then stronger than the second.

From the node exchangeability, one can write :

\begin{align*}
P(N_k^i=n) &= \dbinom{N}{n}P(z_{i\rightarrow 1}=k,z_{i\leftarrow 1}=k,..,z_{i\rightarrow n}=k, z_{i\leftarrow n}=k, \\
 &z_{i\rightarrow n+1}\neq k, z_{i\leftarrow n}\neq k,..,  z_{i\rightarrow N}\neq k, z_{i\leftarrow N}\neq k)
\end{align*}

Again, it follows from the burstiness definition and Bayes' rule :
\begin{equation} \label{eq:memb_ex}
\frac{P(N_k^i=n+1)}{P(N_k^i=n)} = \frac{N-n}{n+1}P(z_{i\rightarrow j}=k,z_{i\leftarrow j}=k | Z_{i\rightarrow}^{-j}, Z_{i\leftarrow}^{-j})	
\end{equation}

The right part of equation \eqref{eq:memb_ex} has known closed form similar to the collaped Gibbs update for the membership assignment of IMMSB (see annexe). The burstiness equation reduce to :

\begin{align*}
\frac{P(N_k^i=n+1)}{P(N_k^i=n)} &= \frac{N-n}{n+1}P(z_{i\rightarrow j}=k | Z_{i\rightarrow}^{-j},) P(z_{i\leftarrow j}=k |  Z_{i\leftarrow}^{-j}) \\
&=  \frac{N-n}{n+1} \left( \frac{n + \alpha_k}{N + \alpha_{\bm{.}}}\right)^2
\end{align*}

\textcolor{red}{Here, there is this polynom of order 2 to solve to find the constraint on $\alpha$ to satisfy the burstiness, I skip it to win some times for now...}

\end{proof}


\subsubsection{Local preferential attachment - General case}~\\

In the previous part we studied the preferential attachment in a context where we fit the model parameters. We showed that according to the type of prior (hard vs soft), it impacted the preferential attachment properties on the random network. To highlight this property we only needed to focus our analysis on the distribution conditioned on what we called $me$, which is actually an estimation of the parameters $F$ and $\Phi$ noted $\hat F$ and $\hat \Phi$. Those learned parameters are drawn from an approximate posterior noted $\hat P(F, \phi | Y, mg) \propto P(Y|F, \Phi)P(F|mg)P(F|mg)$ which came from the inference process such that $me = (\hat F, \hat \Phi) \sim \hat P(F, \phi | Y, mg)$. Thus which approximates the true posterior and provide the following generative equations :

\begin{equation}
    p(Y | \hat F, \hat \Phi) = \prod_{i<j} f_i \Phi f_j^T
\end{equation}

We define the expected degree as :

\begin{equation}
d_{i}^{mg} = \sum_{j\in V} p(y_{ij}=1 | mg)
\end{equation}

It is straightforward that the expected (global) degree is constant for any nodes in the networks for exchangeable models since  the expected link prediction is constant with $N$, and so on for IMMSB and ILFM. 

Nevertheless one can ask about the  expected (local) degree inside a class. The expected local degrees ($mg$ case) given a class, is trivially given from the graphical models and correspond the following quantities  :
\begin{itemize}
	\item $\sum_{j\in V} p(y_{ij}=1  | z_{ij, ij}=k, mg) = N \lambda_1 / (\lambda_1 + \lambda_0)$ for IMMSB,
	\item $\sum_{j\in V} p(y_{ij}=1  | f_{ik, jk}=1, mg)$  for ILFM. Note that we can't compute it in a closed form expression for the same reason that previously of non-conjugacy.
\end{itemize}

In the general case, if we don't know the class a priori, the local degree expectation take the following form :

\begin{itemize}
\item  $d_{i,k}^{mg} = \sum_{j\in V} p(y_{ij}=1, z_{ij, ij}=k | mg)$ for IMMSB,
\item  $d_{i,k}^{mg} = \sum_{j\in V} p(y_{ij}=1, f_{ik, jk}=1| mg)$ for ILFM.
\end{itemize}

Note that it is easy to show we use this definition to define the degree in the $me$ context :
\begin{align*}
 d_{i,k}^{me} &= \sum_{j\in V} p(y_{ij}=1, z{i\rightarrow j}=k, z_{i\leftarrow j =k} | me) \\
  &= L(\phi_{kk} f_{ik} f{jk})
 \end{align*}
 
With the function $L()$ being the sigmoid for ILFM and the identity for IMMSB.

In turns out that again, this expected local degree have a closed form for (i)MMSB, and not for ILFM. For IMMSB we have the following :

\begin{align*}
p(y_{ij}=1, z_{ij, ij}=k | mg) &= p(y_{ij}=1 |  z_{ij, ij}=k, mg) p( z_{ij, ij}=k | mg) \\
 &=E_{\phi_{kk}}[ p(y_{ij}=1 |  z_{ij, ij}=k, \phi_{kk}, mg) ] E_{f_i}[ p( z_{ij}=k | f_i, mg)] E_{f_j}[ p( z_{ji}=k | f_j, mg)] \\
 &= \frac{\lambda_1}{\lambda_0+\lambda_1}\frac{ \alpha_k\alpha_k'}{ (\sum_k \alpha_k)^2}
\end{align*}

Note that from our definitions we have $d_i^{mg} = \sum_k d_{i,k}^{mg}$.


\paragraph{}
\textcolor{red}{Remark on the definition of the burstiness in the $mg$ mode (expected degree):
\hrulefill
\hrulefill
~\\
}

for the burstiness in the "$mg$" with de degree local defined as $d_{i,k}^{mg}$  is trivially bursty, but makes no so much sense for the following reason :

The burstiness equation in this context can be written as :

\begin{equation}
b_n = \frac{p(d_{i,k}^{mg} = n+1)}{p(d_{i,k}^{mg} = n)}  \nearrow_? n 
\end{equation}

We know that $d_{i,k}^{mg} = N \epsilon_k$ which is equivalent to says that $p(d_{i,k}^{mg})$ is a Dirac distribution noted $\delta (N\epsilon_k)$. It means that the burstiness equation is only definite for $n = N\epsilon_k$, (otherwise the denominator is null), and hence its derivative is not defined.


\textcolor{red}{End fo Remark
\hrulefill
\hrulefill
~\\
}

The question of the local burstiness we should ask instead, is by seeking what are the sequence that underly a random variable that we want to analyze Here $d_{i,k}$ should not been seen as the sum over independent marginal probability, but as the joint probability of every draw concerned. 

In a first approach, let's assume we know the relative size of the class $k$ noted $N_i^k$:

\begin{align*}
P(d_{i,k}=n | mg) &= \dbinom{N}{n} P(\{y_{ij}=1, z_{\{ij,ji\}}=k, j\in [1,n]\}, \\ & \{y_{ij}=0, z_{\{ij,ji\}}=k, j\in [1,N_i^k-n]\}, \\
&  \{\ z_{\{ij,ji\}}\neq k, j\in [1,N-N_i^k]\}) \\
\end{align*}


\textcolor{red}{Iam not sure about the number of combination here, I need to think about it.}
