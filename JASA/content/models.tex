\section{Stochastic Mixed Membership Models}
\label{sec:background}

Stochastic mixed membership models are generative models that rely on latent factors (also called latent \textit{classes} or \textit{features}) for modeling relational data such as links in social networks represented by a graph $G=(V,E)$, where $V$ is a set of nodes and $E$ a set of edges between these nodes.

In the remainder, we denote by $N$ the number of nodes in this graph ($N = |V|$) and by $Y$ the adjacency matrix of the graph $G$ ($y_{ij}=1$ if there is a link between nodes $i$ and $j$, $1 \le i,j \le N$; $y_{ij} = 0$ otherwise). Without loss of generality, we assume that the graph is undirected, the directed case being a special case of the undirected one.

Stochastic mixed membership models are characterized by the fact that each node can "belong" to several latent factors, which reflects the fact that each individual usually has several properties, for example can belong to several communities\footnote{As mentioned in \cite{goldenberg2010survey}, the reader should however bear in mind that the notion of latent factors is of stochastic nature and is an approximation of the notions of communities and shared properties.}. The relation between a node $i$ and the latent factors is encoded in a vector denoted $\mat{f}_{i}$, of finite dimension $K$ in standard versions of the models, and of infinite dimension in  non-parametric versions. The collection of all vectors $\mat{f}_{i}$ ($1 \le i \le N$) constitutes the factor matrix $\mat{F}$. Furthermore, a weight matrix $\mat{\Phi}$ is used to encode the relations between the latent factors.

Stochastic mixed membership models differ on the way the vectors $\mat{f}_{i}$ ($1 \le i \le N$) and the matrix $\mat{\Phi}$ are generated. As mentioned before, and to be as general as possible, we consider here the non-parametric versions of the latent feature model \cite{ILFRM}, referred to as \ifm, and of the mixed-membership stochastic block model \cite{iMMSB,fan2015dynamic}, referred to as \imb. This leads to a dynamic number of classes that allows the dimensions of the models to grow with the complexity of the data. This is done in practice by the use of non-parametric prior,  the Indian Buffet Process (IBP) for \ifm\ and the Hierachical Dirichlet Process (HDP)  for \imb. All our results are nevertheless also valid for the finite versions of these models.

In the latent feature model, each node is represented by a finite vector of binary features. The probability of linking two nodes is then based on a weighted similarity between their feature vectors, the weight matrix being generated according to a normal distribution. In its non-parametric version \ifm, the feature vectors are now generated according to an IBP, leading to feature vectors of infinite dimensions (even though only a finite number of dimensions is actually active). The following steps summarize this process:~\\
%
\begin{enumerate}
    \item Generate a feature matrix $\mat{F}_{N \times \infty}$ representing the feature vector of each node: \[\mat{F} \sim \IBP(\alpha)\]
\item Generate a weight matrix for each latent feature:\\
    \[\mat{\phi}_{mn} \sim N(0, \sigma_w), \, m,n \in \mathbb{N}^{+*}\]
\item Generate or not a link between any node $i$ and any node $j$ according to: 
%
\begin{equation*}
y_{ij} \sim \mathrm{Bern}(\sigma(\mat{f}_{i} \mat{\Phi} \mat{f}_{j}^\top))
\end{equation*}
\end{enumerate}
%
where $^\top$ denotes the transpose and  $\sigma()$ is the sigmoid function, mapping $[-\infty, +\infty]$ values to [0,1], and where $y_{ij}$ is a binary variable indicating that a link has been generated ($y_{ij}=1$) or not ($y_{ij}=0$). We will denote by $\mat{Y}$ the $N \times N$ matrix with elements $y_{ij}$. Finally, $\mat{f}_{i}$ denotes the row feature vector corresponding to the $i^{th}$ row of $\mat{F}$.

This model makes use of two real hyper-parameters, one for the IBP process ($\alpha$), and one for the variance of the normal distribution underlying the weight matrix ($\sigma_w$). In the case of undirected networks, the matrices $\mat{Y}$ and $\mat{\Phi}$ are symmetric and only their upper (or lower) diagonal parts are generated. Lastly, both $\mat{F}$ and $\mat{\Phi}$ are infinite matrices. In practice however, one always deals with a finite number of latent features. A graphical representation of this model is given in Figure~\ref{fig:mmm} (left).~\\

The MMSB model generates class membership distributions per node on the basis of a Dirichlet distribution. Then, for each connection between two nodes, a particular class for each node is first sampled from the class membership distribution, and the probability of connecting the two nodes is, as in the previous model, based on a Bernoulli distribution integrating the weight of the two classes. 
~\\
The non-parametric version \imb\ parallels this development but considers, in lieu of the Dirichlet distribution, a Hierarchical Dirichlet Process, leading to the following generative model:
%
\begin{enumerate}
\item Generate the class membership distributions $\mat{F}_{N \times \infty}$:
   \begin{align}
       \bm{\beta} &\sim \gem(\gamma) \nonumber \\
    \mat{f}_i &\sim \DP(\alpha_0, \beta) \quad\text{ for }  i \in \{1, \dotsc, N\} \nonumber
   \end{align}
where $\gem$ (named after Griffiths, Engen and McCloskey) denotes the Stick Breaking Process distribution over the set of natural numbers and $\DP$ a Dirichlet Process \cite{HDP};
\item Generate a weight matrix for each latent class from i.i.d Beta distribution:\\
\[ \phi_{mn} \sim \mathrm{Beta}(\lambda_0,\lambda_1), \, m,n \in \mathbb{N}^{+*} \]
\item For any node $i$ and any node $j$, choose a class from their class membership distribution according to a Categorical distribution and generate or not a link according to a Bernoulli distribution:
   \begin{gather*}
    z_{i \rightarrow j} \sim \mbox{Cat}(\mat{f}_i) \ , \quad z_{i \leftarrow j} \sim \mbox{Cat}(\mat{f}_j) \\
    y_{ij} \sim \mathrm{Bern}(\phi_{z_{i \rightarrow j}z_{i \leftarrow j}})
   \end{gather*}
\end{enumerate}
%
We have this time four real hyper-parameters, two for the Hierarchical Dirichlet Process ($\gamma$ and $\alpha_0$) and two for the Beta distribution underlying the weight matrix ($\lambda_0$ and $\lambda_1$). As for the previous model, in the case of undirected networks, the matrices $\mat{Y}$ and $\mat{\Phi}$ are symmetric and only their upper (or lower) diagonal parts are generated; as before again, both $\mat{F}$ and $\mat{\Phi}$ are infinite matrices. A graphical representation of this model is given in Figure~\ref{fig:mmm}~(right).

\begin{figure}[t]
	\centering
	\minipage{0.45\textwidth}\vspace{1cm}
	\scalebox{0.88}{
	\input{\lpath/img/ilfrm2.tex}}
	\endminipage
	\minipage{0.45\textwidth}
	\scalebox{0.88}{
		\input{\lpath/img/mmsb2.tex}}
	\endminipage
	\caption{The two graphical representations of (left) the latent feature model and (right) the latent class model. The difference between the two graphical structures of  models lies in the way representations are associated to nodes: a fixed representation is used in the case of the latent feature model, whereas the representation in the latent class model varies according to the link considered.}
	\label{fig:mmm}
\end{figure}

\paragraph{Settings} In a Bayesian context, the set of hyper-parameters underlying the model considered is known. This set, denoted $\mg$, respectively corresponds to $\alpha$ and $\sigma_w$ for \ifm\ and to $\gamma$,  $\alpha_0$, $\lambda_0$ and $\lambda_1$ for \imb. For mixed membership models, the evidence $\p(Y|\mg)$ has no closed-form solution. Yet, the random graph $G$ is exchangeable so that, for any permutation $\pi$ on integers, one has:
%
\[
P((y_{ij})_{i,j\in \mathcal{R}} | \mg) = P((y_{\pi(i)\pi(j)})_{i,j\in \mathcal{R}} | \mg)
\]
%
and one can generate networks from $\mg$ by following the generative processes described above for \ifm\ and \imb. In this setting, the question we ask ourselves is whether the networks generated from $\mg$ comply with the preferential attachment effect.

However, the typical use of the above models corresponds to the scenario in which some observations (\textit{i.e.} an existing network, observed till a certain time) are available and are used to estimate $\mat{F}$ and $\mat{\Phi}$ from which new links are created. The estimation of $\mat{F}$ and $\mat{\Phi}$ is based on:
%
\begin{equation}
    \p(F, \Phi |Â Y, \mg) = \frac{\p(Y|F,\Phi)\p(F|\mg)\p(\Phi|\mg)}{\p(Y|\mg)}
\end{equation}
%
and usually makes use of standard Gibbs sampling and Metropolis-Hastings algorithms\footnote{We do not detail the inference of  $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$ here and refer the interested reader to \cite{ILFRM,IBP,HDP,fan2015dynamic}.}

In the remainder, we denote by $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$, for both \ifm\ and \imb, the estimates of $\mat{F}$ and $\mat{\Phi}$ obtained from $\mg$ and $Y$, and furthermore set $\me = \{\mat{\hat{F}},\mat{\hat{\Phi}}\}$. Whether, from the learned parameters $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$, the new links generated produce networks that comply with the preferential attachment effect is the second question we ask ourselves in this study.

We now propose a formalization of preferential attachment in social networks and answer the above questions.

%In the rest of this paper, we will refers to both context in which we study a Bayesian model, and we recall here the fundamental property of each one :
%\begin{itemize}
%    \item $\mg$ that represents the set of hyperparameters of $\mathcal{M}$, such that the random graph $G$ is exchangeable, thus for any permutation $\pi$ on integers, one has: \[ P((y_{ij})_{i,j\in \mathcal{R}} | \mg) = P((y_{\pi(i)\pi(j)})_{i,j\in \mathcal{R}} | \mg) \]
%    \item $\me$ that represents the set of elements ($\bm{F}$ and $\bm{\Phi}$) \textbf{ CL :  $\me = \{\hat F,  \hat \Phi \}$ ?} of $\mathcal{M}$ such that interactions are conditionally independent:  \[P((y_{ij})_{i,j\in \mathcal{R}} | \me) = \prod_{i,j\in \mathcal{R}} P(y_{ij} | \me)  \]
%\end{itemize}
