\documentclass{llncs}


\newcommand*{\lpath}{./}%
%\usepackage[cmex10]{amsmath, mathtools}
%\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{amsmath,amssymb,amsbsy,amsfonts}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{url}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{fancyvrb}
\usepackage{yfonts}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\usepackage{calc}%    For the \widthof macro
\usepackage{xparse}%  For \NewDocumentCommand


%\input{./header.tex}
%%%%%%%%%% Math
\renewcommand{\text}{\textnormal}
\newcommand{\ifm}{\texttt{ILFM}}
\newcommand{\imb}{\texttt{IMMSB}}
\newcommand{\pr}{P}
\newcommand{\p}{P}
\newcommand{\E}{\mathbb{E}}
\newcommand{\divkk}{\mathbb{K}}
\newcommand{\entropy}{\mathbb{H}}
\newcommand{\gem}{\mathrm{GEM}}
\newcommand{\Mult}{\mathrm{Mult}}
\newcommand{\DP}{\mathrm{DP}}
\newcommand{\IBP}{\mathrm{IBP}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\unit}{1\!\!1}

%\newtheorem{definition}{Definition}[section]
%\newtheorem{proposition}{Proposition}[section]
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{corollary}{Corollary}[section]


\title{ Stochastic mixed membership models and link prediction: a study of homophily and preferential attachment in social networks}




%\keywords{Stochastic mixed membership models $|$ Social networks $|$ Homophily $|$ Preferential attachment} 

%\begin{abstract}
%We assess here whether standard stochastic mixed membership models are adapted for link prediction in social networks by studying how they handle homophily and preferential attachment. To do so, we first introduce formal definitions of these phenomena; we then study how stochastic mixed membership models relate to these definitions. Our theoretical analysis reveals that standard stochastic mixed membership models comply with homophily with the similarity that underlies them. For preferential attachment, the situation is more contrasted: if these models do not comply with global preferential attachment, their compliance to local preferential attachment depends on whether the memberships to latent factors are hard or soft, and in the latter case on whether the underlying latent factor distribution is bursty or not. We illustrate these elements on synthetic and real networks.
%\end{abstract}


\begin{document}


\maketitle

\label{sec:intro}

Several powerful relational learning models have been proposed to solve the problem commonly referred to as \textit{link prediction} that consists in predicting the likelihood of a future association between two nodes in a network \cite{LibenNowell07,HassanZaki11}. Among such models, the class of stochastic mixed membership models has received much attention as such models can be used to discover hidden properties and infer new links in social networks. Two main models in this class have been proposed and studied in the literature: the latent feature model \cite{BMF} and its non-parametric extension \cite{ILFRM}, and the mixed-membership stochastic block model \cite{MMSB}, and its non parametric extension \cite{iMMSB,diMMSB}.

Although drawn from a wide range of domains, real world social networks exhibit general properties. In this work, we focus on two important such properties, namely \textit{homophily} and \textit{preferential attachment} \cite{Newman2010,Barabasi2003}, and assess to which extent link prediction models, as the ones mentioned above, comply with them.

Stochastic mixed membership models are generative models that rely on latent factors (also called latent \textit{classes} or \textit{features}) that represent hidden properties of the nodes of the graph $G=(V,E)$ associated to the social network (each node of the graph represents an individual in the social network); in the remainder, we denote by $N$ the number of nodes in this graph ($N=|V|$). Stochastic mixed membership models are characterized by the fact that each node can "belong" to several latent factors, which reflects the fact that each individual usually has several properties, for example can belong to several communities\footnote{As mentioned in \cite{goldenberg2010survey}, the reader should however bear in mind that the notion of latent factors is of stochastic nature and is an approximation of the notions of communities and shared properties.}. The relation between a node $i$ and the latent factors is encoded in a vector denoted $\mat{f}_{i}$, of finite dimension $K$ in standard versions of the models, and of infinite dimension in  non-parametric versions. The collection of all vectors $\mat{f}_{i}$ ($1 \le i \le N$) constitutes the factor matrix $\mat{F}$. Furthermore, a weight matrix $\mat{\Phi}$ is used to encode correlations between the latent factors.


Stochastic mixed membership models differ on the way the vectors $\mat{f}_{i}$ ($1 \le i \le N$) and the matrix $\mat{\Phi}$ are generated. As mentioned before, and to be as general as possible, we consider here the non-parametric versions of the latent feature model \cite{ILFRM}, referred to as \ifm, and of the mixed-membership stochastic block model \cite{iMMSB,diMMSB}, referred to as \imb. All our results are nevertheless also valid for the finite versions of these models.

In \ifm, the factor matrix $\mat{F}$ is generated by an Indian Buffet Process, and each element of the matrix $\mat{\Phi}$ is generated according to a normal distribution with 0 mean and fixed, common variance. The Indian Buffet Process yields binary vectors; the $k^{th}$ coordinate of the vector $\mat{f}_{i}$, denoted $f_{ik}$, is thus either $1$ or $0$, meaning that node $i$ belongs or not to the $k^{\mbox{th}}$ latent factor. In \imb, the factor matrix $\mat{F}$ is obtained with a Hierarchical  Dirichlet Process, and each element of the matrix $\mat{\Phi}$ is generated according to a Beta distribution with fixed, common parameters. The Hierarchical  Dirichlet Process yields this time membership probabilities; $f_{ik}$ directly encodes the probability that node $i$ belongs to the $k^{th}$ latent factor. We refer the interested reader to \cite{ILFRM} and \cite{iMMSB} for more details on these processes.

Once the factor and weight matrices have been generated, the probability to generate a link between any two nodes $i$ and $j$ for \ifm\ is given by:
%
\begin{align}
P(y_{ij}=1|\mat{F},\mat{\Phi}) = \sigma(\mat{f}_{i} \mat{\Phi} \mat{f}_{j}^\top)
\label{eq:ilfm}
\end{align}
%
where $\sigma()$ is the sigmoid function and $\top$ denotes the transpose. In \imb, one has first to select two latent factors from $\mat{f}_i$ and $\mat{f}_j$, and then generate or not a link between $i$ and $j$. This amounts to: (a) choose a class from the class membership distributions according to a categorical distribution:
%
\begin{align}
 z_{i \rightarrow j} &\sim \mbox{Cat}(\mat{f}_i) \nonumber \\
 z_{i \leftarrow j} &\sim \mbox{Cat}(\mat{f}_j) \nonumber
\end{align} 
%
and (b) generate a link with probability:
%
\begin{align}
 P(y_{ij}=1|,\mat{\Phi}) = \phi_{z_{i \rightarrow j}z_{i \leftarrow j}}
\label{eq:immsb}
\end{align}
%
where $\phi_{kk'}$ denotes the element of $\mat{\Phi}$ at row $k$, column $k'$.

In the typical use of the above models for link prediction, some observations (\textit{i.e.} an existing network, observed till a certain time) are available and are used to estimate $\mat{F}$ and $\mat{\Phi}$, from which new links are predicted. In the remainder, we denote by $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$ the estimates of $\mat{F}$ and $\mat{\Phi}$, that are usually obtained through standard (collapsed) Gibbs sampling and Metropolis-Hastings algorithms \cite{ILFRM,IBP,HDP,diMMSB}. We furthermore denote by $\mathcal{M}_e$ the version of both \ifm\ and \imb\ models in which $\mat{F}$ and $\mat{\Phi}$ are assumed known and fixed to $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$. We now investigate whether, from the learned parameters $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$, the new links generated produce a network that comply with homophily and preferential attachment.

\paragraph{\underline{Significant Statement}}We introduce formal definitions of the compliance of probabilistic link prediction models to homophily and preferential attachment in social networks, and show that standard stochastic mixed membership models \ref{fig:mmm} comply with homophily with the similarity that underlies them. For preferential attachment, the situation is more contrasted: if these models do not comply with global preferential attachment, their compliance to local preferential attachment depends on whether the memberships to latent factors are hard or soft, and in the latter case on whether the underlying latent factor distribution is bursty or not.

\begin{figure}[t]
	\centering
	\minipage{0.45\textwidth}\vspace{1cm}
	\scalebox{0.88}{
	\input{\lpath/img/ilfrm2.tex}}
	\endminipage
	\minipage{0.45\textwidth}
	\scalebox{0.88}{
		\input{\lpath/img/mmsb2.tex}}
	\endminipage
	\caption{The two graphical representations of (left) the latent feature model and (right) the latent class model. The difference between the two models lies in the way representations are associated to nodes: a fixed representation is used in the case of the latent feature model, whereas the representation in the latent class model varies according to the link considered.}
	\label{fig:mmm}
\end{figure}


\section{Related Work}

%Burstiness on topic model:
%Modeling Word Burstiness Using the Dirichlet Distribution (DCM)
%Accounting for Burstiness in Topic Models (DCMLDA)
%Proposal of a-MMSB in : Scalable Inference of Overlapping Communities with high diagonal only...

%to read: Stochastic blockmodels and community structure in networks

Recently, the MMSB \cite{MMSB} and ILFM \cite{ILFRM} models have been successfully used for link predictions and structure discovery in social networks. Several extensions, has been proposed. In \cite{AMMSB}, they proposed a adaptation of a MMSB model that scale well using stochastic variational inference and use it for the discovery of overlapping communities in big networks (millions of nodes). They constrained the weight matrix to have weights in the diagonal and a fixed small value elsewhere and named it a-MMSB where "a" stand for assortative. The MMSB model has also been extended to a nonparametric dynamic version to handle temporal networks \cite{fan2015dynamic}. ILFM has also been extended in several way; to handle non-negative weights in \cite{IMRM} and within a more subtle latent feature structure in \cite{ILAM}. Nevertheless the characterization of models with regards to the properties for networks remains to be explored \cite{jacobs2014unified}. Theoretical results on the structure of random graph under exchangeability assumptions are however well reported in \cite{orbanz2015bayesian}, although the characterization of social networks properties within a Bayesian framework is not covered. In \cite{hoff2008modeling}, the role of exchangeability was first pointed out and its relation to the data representation where they provide a comparative study of the homophily and structural equivalence in different model settings, but without a formal definition of both. An other important mention of the exchangeability assumptions, was done in \cite{bickel2009nonparametric} where they characterize the asymptotic comportment of the likelihood in the blockmodel settings, showing that the inference of the likelihood lead to a consistent estimation of the community structure. But they only cover the case a single membership.


\section{Homophily: \emph{"Birds of a feather flock together"}}
\label{sec:homophily}

Homophily refers to the tendency of individuals to connect to similar others: two individuals (and thus their corresponding nodes in a social network) are more likely to be connected if they share common characteristics~\cite{mcpherson2001birds,lazarsfeld1954friendship}. The characteristics often considered are inherent to the individuals and may represent their social status, their preferences or their interest. A related notion is the one of {\it assortativity}, that is slightly more general as it applies to any network, and not just social networks, and refers to the tendency of nodes in networks to be connected to others that are similar in some way.

A definition of homophily has been proposed in~\cite{la2010randomization}. However, this definition, which relies on a single characteristic (like age or gender), does not allow one to assess whether latent models for link prediction capture the homophily effect or not. We thus introduce a new definition of homophily:
%
\begin{definition}[Homophily] \label{def:homophily}
	Let $\mathcal{M}_e$ be a probabilistic link prediction model and $s$ a similarity measure between nodes. We say that $\mathcal{M}_e$ is homophilic under the similarity $s$ iff, $\forall (i,j,i',j') \in V^4$:
%
\begin{equation}
s(i,j) > s(i',j')  \implies \pr(y_{ij}=1 \mid \mathcal{M}_e) > \pr(y_{i'j'}=1  \mid \mathcal{M}_e) \nonumber
\end{equation}
%
\end{definition}
%
\noindent As one can note, this definition directly captures the effect "if two nodes are more similar, then they are more likely to be connected". 

Different similarities can be considered, as long as they are based on the proximity of the properties of the nodes considered. In stochastic mixed membership models, these properties are encoded in the latent factors. Indeed, as mentioned before, the factor matrix $\mat{\hat{F}}$ aims at capturing some latent properties of the nodes, whereas the estimated matrix $\mat{\hat{\Phi}}$ captures the correlations between these latent properties. One can thus define, on their basis, a "natural" similarity between nodes as follows:
%
\begin{equation}
s_n(i,j) = \mat{\hat{f}}_{i} \mat{\hat{\Phi}} \mat{\hat{f}}_j^\top \nonumber
\end{equation}
%
It is straightforward that both \ifm\ and \imb\ in the setting $\mathcal{M}_e$ are homophilic with respect to $s_n$. Indeed, $\pr(y_{ij}=1 \mid \mathcal{M}_e)$ increases with $s_n$ for \ifm\ as the sigmoid function is strictly increasing (Eq.~\ref{eq:ilfm}). Furthermore, marginalizing over the $z$ variables in \imb\ leads to:
%
\begin{align}
\pr(y_{ij} =1 \mid \mathcal{M}_e) & = \sum_{k,k'} \hat{\phi}_{k,k'} \pr(z_{i \rightarrow j}=k | \mathcal{M}_e) \pr(z_{i \leftarrow j}=k' | \mathcal{M}_e) \nonumber \\
& = \sum_{k,k'} \hat{\phi}_{k,k'} \hat{f}_{ik} \hat{f}_{jk'} = \mat{\hat{f}}_{i} \mat{\hat{\Phi}} \mat{\hat{f}}_j^\top \nonumber
\end{align}

Dropping the correlation between latent factors in the natural similarity leads to a new similarity, solely based on the latent factors and defined by $s_l(i,j) = \mat{\hat{f}}_{i} \mat{\hat{f}}_j^\top \nonumber$ ($s_l$ stands for latent similarity). With this similarity, however, neither \ifm\  nor \imb\ are homophilic. Indeed, let us first assume that $\mat{\hat{\Phi}}$ is null on the diagonal, and strictly positive elsewhere (this can be obtained for both models). For \imb, one has:
%
\begin{align}
\pr(y_{ij}=1 \mid \M_e) & = \sum_{k' \neq k} \hat{f}_{ik} \hat{\phi}_{kk'} \hat{f}_{jk'} \nonumber 
\end{align}
%
as $\hat{\phi}_{kk} = 0$. Let us now consider $\mat{\hat{f}}_i=\mat{\hat{f}}_j=(0,1,0)$ and $\mat{\hat{f}}_{i'}=(0.5,0,0.5)$ and $\mat{\hat{f}}_{j'}=(0,1,0)$. Then, $s_l(i,j)=1$ and $s_l(i',j')=0$. However, $\pr(y_{ij}=1 \mid \M_e) = 0$ whereas $\pr(y_{i'j'}=1 \mid \M_e) > 0$. \imb\ is thus not homophilic under $s_l$. The same example, replacing $\mat{\hat{f}}_{i'}=(0.5,0,0.5)$ by $\mat{\hat{f}}_{i'}=(1,0,1)$, can be used to show that \ifm\ is neither homophilic under $s_l$.

This shows that, for a model to be homophilic, it should be designed according to the similarity at the basis of the proximity between individuals. Both \ifm\ and \imb\ have been designed on the basis of the natural similarity $s_n$, and directly encode the fact that similar nodes, according to $s_n$, are more likely to connected.  It is furthermore possible to make these models homophilic under $s_l$ by imposing constraints on the weight matrix $\mat{\Phi}$ (and hence its estimate $\mat{\hat{\Phi}}$); for example, considering positive, diagonal matrices with equal values on the diagonal leads to homophilic models. In that case, the latent factors can be interpreted as community indicators, each community being of equal importance. This is in line with what is done in the study presented in \cite{AMMSB} to find overlapping communities through assortativity constraints in the mixed membership stochastic block model.

\section{Preferential attachment: \emph{"The rich get richer"}}
\label{sec:burstiness}

Preferential attachment, sometimes referred to as the \textit{rich get richer} rule, is a mechanism according to which each node is connected to an existing node with a probability that increases with the number of links of the chosen node\footnote{This property is well captured by a power law distribution, hence the claim often made that preferential attachment translates as a power law for the node degrees distribution.}. However, as noted in Leskovec \textit{et al.}, usually, in social networks, entities do not have a global knowledge of the network. The preferential attachment model is thus more likely to be local, and to be specific to communities \cite{LeskovecBKT08}.

Preferential attachment relates to a general phenomenon known as \textit{burstiness}\footnote{A.L. Barab\'asi, for example, uses the term \textit{preferential attachment} in \cite{barabasi1999emergence}, and \textit{burstiness} in \cite{barabasi_burst}.} which describes the fact that some events appear in bursts, \textit{i.e.} once they appear, they are more likely to appear again. Burstiness has been studied in different fields, in particular in computational linguistics and information retrieval to characterize word occurrences \cite{church1995poisson}. In these domains, simple definitions of burstiness have been proposed \cite{clinchant2008bnb,clinchant2010information}, for both discrete and continuous probability distributions. They directly capture the fact that a probability distribution is bursty if the probability of generating a new occurrence of an event increases with the number of occurrences of this event. We adapt here these definitions for preferential attachment in social networks.

In the context of social networks, the notion of preferential attachment amounts to the fact that the more links a node has (\textit{i.e.} the higher its degree), the more likely it will be linked to new nodes. As mentioned before, this phenomenon however appears at different levels: globally for the whole network, and locally within classes. For global preferential attachment, the degree of a node is a known integer; for local preferential attachment, in most models, the exact degree is not known, and one has to rely on an estimate of it, as done in the following definition:
%
\begin{definition}[Preferential attachment]
Let $i$ be a node in a social network and let $d_i$ denote its degree. 
\begin{description}
 \item[(1)] \emph{Global Preferential Attachment}: we say that a probabilistic link prediction model $\mathcal{M}_e$ satisfies the global preferential attachment iff, for any node $i$, $\pr(d_i \ge n+1 \mid d_i \ge n, \mathcal{M}_e)$ increases with $n \in \mathbb{N}$;
 \item[(2)] \emph{Local Preferential Attachment}: we say that a probabilistic link prediction model $\mathcal{M}_e$ satisfies the local preferential attachment iff, for any node $i$, denoting $d_{i,k}$ the degree of node $i$ in class $k$, $\forall \epsilon \in [0,1], \, \pr(d_{i,k} \ge x+\epsilon \mid d_{i,k} \ge x, \mathcal{M}_e)$ increases with $x \in [0,N-1]$. Furthermore, $d_{i,k}$ is defined as the expectation, over all nodes in the network, of forming a link through latent factor $k$.
\end{description}
\label{def:burst-soc-net}
\end{definition}
%
As one can note, these definitions directly translate the fact that "the more connections a node has, the more likely it is to be connected to new nodes". The only difference between the local and global cases is that the degree is usually unknown in the local case, and is here estimated through its expectation.

For global preferential attachment, the degree $d_i$ directly corresponds to the number of outgoing links of node $i$. Exploiting the fact that the observations are independent given $\mat{\hat{F}}$ and $\mat{\hat{\Phi}}$, one has:
%
\begin{align}
\pr(d_{i} \ge n+1 \mid d_{i} \ge n, \mathcal{M}_e) = 1 - \prod_{j \notin \mathcal{V}(i)} p(y_{ij} = 0 \mid d_{i} \ge n, \mathcal{M}_e) \nonumber \\
= 1 - \prod_{j \notin \mathcal{V}(i)} (1 - p(y_{ij} = 1 \mid d_{i} \ge n, \mathcal{M}_e)) \nonumber
\end{align}
%
where $\mathcal{V}(i)$ denotes the set of nodes connected to node $i$. Let $c=\min_{j \in V}  (1-p(y_{ij} = 1 \mid d_{i} \ge n, \mathcal{M}_e))$. One has:
%
\[
0 \le \pr(d_{i} \ge n+1 \mid d_{i} \ge n, \mathcal{M}_e) \le (1 - c^{N-n}) \xrightarrow[n \rightarrow N]{} 0
\]
%
which shows that $\pr(d_{i} \ge n+1 \mid d_{i} \ge n, \mathcal{M}_e)$ does not increase with $n$. We thus have the following property:
%
\begin{proposition}[]
\label{pref-attch-glob}
Both \ifm\ and \imb\ do not satisfy global preferential attachment.
\end{proposition}
%
For local preferential attachment, the situation is slightly more complex:
%
\begin{proposition}[]
\label{pref-attch-loc}
\imb\ satisfies local preferential attachment whereas \ifm\ does not.
\end{proposition}
%
\noindent \textbf{Proof (sketch)} Let $y_{ij,k}$ be the binary random variable that is $1$ if nodes $i$ and $j$ are linked through the latent factor $k$ and $0$ otherwise. Then, $d_{i,k} = \sum_{j \in V} \pr(y_{ij,k} =1 | \mathcal{M}_e)$. For \imb, this leads to $d_{i,k} = \sum_{j \in V} \hat{f}_{ik} \hat{\Phi}_{kk} \hat{f}_{jk} = \hat{f}_{ik} \sum_{j \in V} \hat{\Phi}_{kk} \hat{f}_{jk}$. The positive reinforcement effect of the Dirichlet Process \cite{HDP} at the basis of \imb\ corresponds to a burstiness phenomenon and directly translates, for any $i$ and any $k$, as: $\pr(\hat{f}_{ik} \ge x'+\epsilon' \mid \hat{f}_{ik} \ge x',\mathcal{M}_e)$ increases with $x'$ (for all $\epsilon'$ and $x'$ chosen according to the domain of definition of $\hat{f}_{ik}$). Setting $x=x'(\sum_{j\in V} \hat{\Phi}_{kk} \hat{f}_{jk})$ and $\epsilon = \epsilon'(\sum_{j\in V} \hat{\Phi}_{kk} \hat{f}_{jk})$ and exploiting the fact that $\sum_{j\in V} \hat{\Phi}_{kk} \hat{f}_{jk}$ is positive and independent of $i$ leads to: $\pr(d_{i,k} \ge x+\epsilon \mid d_{i,k} \ge x, \mathcal{M}_e)$ increases with $x$, which proves that \imb\ satisfies the local preferential attachment effect. For \ifm,  let $C_{i,k} = |\{j \in V, \hat{f}_{jk} = \hat{f}_{ik} = 1\}|$. As the factor matrix is binary, one has:
%
\[ 
d_{i,k} = \sum_{j\in V} \sigma(\hat{f}_{ik} \hat{\Phi}_{kk} \hat{f}_{jk}) =  C_{i,k} (\sigma(\hat{\Phi}_{kk})-0.5) + \frac{N}{2}
\]
%
As $\hat{f}_{ik}$ is binary, there is no positive reinforcement effect: $C_{i,k}$ does not increase if $\hat{f}_{ik}=1$, thus \ifm\ does not satisfy local preferential attachment. \hspace{4.69cm} $\Box$

The above propositions show that both models are deficient in the sense that they do not guarantee that the networks they generate will comply to the global (and local in case of \ifm) preferential attachment phenomena, which are inherent properties of the probability distributions underlying the models. This does not mean however that \ifm\ and \imb\ are not able to well model social networks during the learning phase, even according to the underlying degree distribution. Indeed, the Gibbs updates for both models will assign higher weight to nodes and factors that have been encountered more often during the learning phase. Provided there is enough training data, both models will likely reproduce the degree distributions observed in the training data. We will observe that in the following section, devoted to the illustration of the properties we have established.


\section{Preferential Attachments: Extensions and generalization}
In this section we provide an extension of the study of the preferential attachment in two directions :
\begin{itemize}
    \item Recall the representation theorem \cite{orbanz2015bayesian} about exchangeable sequence and arrays and it's consequences on random networks.%Show the relation between exchangeable sequence (jointly exchangeable adjacency matrix) and the burstiness phenomenon associated to the sum over realization of the exchangeable sequence,
    \item Characterizing the burstiness phenomenon over the size of the classes that the models produce in their generative process (feature burstiness). 
    \item Specifying the limitation of studying the preferential attachment in the $M_e$ context by specifying it's relation with $M_g$. The $M_g$ context represents the models seen from it's hyper-parameters while $M_e = (\hat F, \hat \Phi)$ represents the model seen from an estimated model (with a inference procedure like a MCMC or a variational inference).
\end{itemize}

\subsubsection{Recall of Definitions and Theorems on Exchangeability}~\\

An exchangeable sequence is a sequence $X := (X_1 \in A_1, X_2 \in A_2,...)$ of random variables whose joint distribution satisfies :
\begin{equation}\label{eq:exch}
P(X_1 \in A_1, X_2 \in A_2,...) = P(X_{\pi(1)} \in A_1, X_{\pi(2)} \in A_2,...)
\end{equation}

for every permutation $\pi$  of integers and collection $A_1, A_2,...$ of measurable sets. Following the notation in \cite{orbanz2015bayesian} we can express \eqref{eq:exch} by $P(X_n) = P(X_{\pi(n)})$. In order to characterize the adjacency matrix underlying a social network, we can extend the definition of sequence by denoting a random matrix as $(X_{ij})_{i,j \in \mathbb{N}}$. Thus we says that the random matrix $X$ is jointly exchangeable if it satisfies :
	\begin{equation}
	P(X_{ij}) = P(X_{\pi(i)\pi(j)})
	\end{equation}
	
The Aldous-Hoover theorem generalizes the representation theorem for exchangeable sequence of De-Finetti for exchangeable matrix. It states that a random matrix $(X_{ij})$ is a jointly exchangeable matrix if and only if it can be represented as follows : There is a random measurable function $F:[0,1]^3 \rightarrow X$ such that : 
\begin{equation}
X_{ij} \sim F(U_i, U_j, U_{\{\{i,j\}\}})
\end{equation}

where $(U_i)_{i\in \mathbb(N)}$ and $(U_{\{\{i,j\}\}})_{i, j\in \mathbb(N)}$ are, respectively, a sequence and matrix of i.i.d. Uniform[0,1] random variables.

A important consequence of the Aldous-Hoover theorem is that an exchangeable graph is either empty or dense. It means that if the graph is not empty, the number of edges $n$ will be lower bounded by $cn^2$ with $c$ a constant. This  come from the the fact that edge proportion is independent of $N$ and can be computed as follows $\epsilon = \int_{[0,1]^3} F(x, y, z)dxdydz$.

~\\
We illustrate this results for (i)MMSB model (for the nonparametric version, note that the support of $k$, and $\bm\alpha$ is theoretically infinite and in the latter drawn from a stick breaking process  $\bm \alpha \sim SB(\gamma)$), by marginalizing over the model parameters, we can compute the expected proportion of edges in the graph $G$ :

\begin{align*}
&p(y_{ij}=1| \bm{\alpha}, \bm{\lambda}) = \int_{f_i} \int_{f_j} \int_{\Phi} p(f_i| \alpha ) p(f_j| \alpha)p(\Phi| \lambda) \\
&\times \sum_{k,k'} p(y_{ij} \mid \phi_{kk'})\ p(k\mid f_i)p(k'\mid f_j)df_i df_j d\Phi \\
&=  \sum_{k,k'} \int_{\Phi} \phi_{kk'} p(\Phi| \lambda) d\Phi \int_{f_i} f_{ik} p(f_i| \alpha )df_i \int_{f_j} f_{jk}  p(f_j| \alpha ) df_j \\
&= \sum_{k,k'} \E[B(\lambda_1, \lambda_0)] \E_k[Dir(\mat{\alpha} )] \E_{k'}[Dir(\mat{\alpha})] \\
&= \frac{\lambda_1}{\lambda_0+\lambda_1}\frac{\sum_{k,k'} \alpha_k\alpha_k'}{(\sum_k \alpha_k)^2} = \epsilon
\end{align*}

It follows that this expectation do not depend on the number of vertex $n$, and the expected number of edges for an undirected graph is $\dbinom{n}{2} \epsilon = O(n^2)$.

Note that for ILFM, the expected proportion of edges in the graph don't take a closed form expression since the (kernel) likelihood is not conjugate with the Indian Buffet Process :

\begin{equation}
P(y_{ij}=1 | \alpha, \sigma) = \int_\Phi \sum_F p(y_{ij}=1 | \Phi, F)P(F | \alpha) P(\Phi|\sigma) d\Phi
\end{equation}

There is no closed form for this expression due to it's non-conjugacy.

Here is the different component of this previous equation for completeness \cite{griffiths2011indian} :
\begin{equation}
p(y_{ij}=1 | \Phi, F) = \sigma(f_i\Phi f_j^T)
\end{equation}

\begin{equation}
p(\Phi|\sigma) = \prod_{k, k'} P(\phi_{k,k'}|\sigma) \qquad \mathrm{with} \quad P(\phi_{k,k'}|\sigma) = N(0, \sigma)
\end{equation}

\begin{equation}
P(F \mid \alpha) = \frac{\alpha^{K_+}}{\prod_{i=1}^N K_1^{(i)} } \exp(-\alpha H_N) \prod_{k=1}^{K_+} \frac{(N - m_k)!(m_k - 1)!}{N!}
\end{equation}

Where $H_N = \sum_{j=1}^N \frac{1}{j}$ is the Nth harmonic number, $m_k$ the number of observations containing feature $k$ and $K_+=K-K_0$ is the number of represented features where $K_0$ is the number of observation for which $m_k=0$.



\subsubsection{Feature burstiness}~\\

We propose a third definition for the study of the preferential attachment in social networks. It concerns the preference that an individuals would have to join a class given the current number of actual members. Thus the quantity we study here correspond to the size of the classes : thus we add the following third definition to the previous section : 

To pursue this definition, one need to precise what precisely means the "size of a class". Depending on what is the definition of the membership within a class that we consider, the notion of size will differs.

We consider the two following cases, which defined the relative size $N_k^i$ for any node $i$. It represents the size of class from a node point a view and can be interpreted as the potential number of nodes that a given node $i$ can bind to :
\begin{itemize}
    \item Soft membership - IMMSB : In this case, nodes belongs to every classes in different proportion. The size of class refers in this case to the number of relation (linked or non-linked) within a class who depends on the random tensor $Z$ : $N_k^i = \sum_{j \in V} \delta(z_{i\rightarrow j}=k)\delta(z_{j\rightarrow i}=k)$,
    \item Hard membership - ILFM : This case correspond to the number of nodes falling joitly in a class since the membership is in the binary set \{ \emph{belongs} or \emph{not belongs} \}, which depends on the random matrix $F$ : $N_k^i = \sum_{j\in V} f_{ik}f_{jk}$.
\end{itemize}

From that definition, one can compute the size of a class as $N_k = \sum_{i\in V} N_k^i$ and the following upper bounds hold for an undirected network model with self loop: $N_k^i \leq N$ and $N_k \leq \dbinom{N}{2}$.

The question that arise is how are distributed the $N_{1:k}$ random variables and what is their bursty dynamic.

\begin{definition}[Preferential attachment (for class size)]
Let $k$ be a class in a social network and let $N_k$ denote its size. 
\begin{description}
    \item[(3)] \emph{feature preferential attachment}: we say that a probabilistic link prediction model $M_g$ satisfies the feature preferential attachment iff, for any latent class $k$, the model satisfy the following , $\pr(N_k \ge n+1 \mid N_k \ge n, \mathcal{M}_g)$ increases with $n \in [0,\dbinom{N}{2}]$.
\end{description}
\label{def:burst-soc-net2}
\end{definition}


\begin{proposition}
Both ILFM and IMMSB respect the feature preferential attachment (with regularities conditions on hyperparameters for IMMSB).	
\end{proposition}

\begin{proof}[sketch]
	
For ILFM:

From node exchangeability on can write :
\begin{equation} \label{eq:feat_ex}
P(N_k=n) = \dbinom{N}{n}P(f_{1k}=1,..,f_{nk}=1,f_{(n+1)k}=0,..,f_{Nk}=0)
\end{equation}

It follows from the burstiness definition and Bayes' rule :
\begin{equation}
\frac{P(N_k=n+1)}{P(N_k=n)} = \frac{N-n}{n+1}P(f_{ik}=1 | F_{.k}^{-ik})
\end{equation} 

Here, the node index $i$ is a free variable since the sequence in equation \eqref{eq:feat_ex} is exchangeable, who can choose any nodes.

As $F$ is drawn from an Indian Buffet Process, we have $P(f_{ik}=1 | F_{.k}^{-i}) = \frac{n}{N}$.\\


For IMMSB: 

Remark: note if $N_k^i$ is bursty, it implies that $N_k$ is also busrty from its definition because the sum conserve the inequality. And in the former case the burstiness is then stronger than the second.

From the node exchangeability, one can write :

\begin{align*}
P(N_k^i=n) &= \dbinom{N}{n}P(z_{i\rightarrow 1}=k,z_{i\leftarrow 1}=k,..,z_{i\rightarrow n}=k, z_{i\leftarrow n}=k, \\
 &z_{i\rightarrow n+1}\neq k, z_{i\leftarrow n}\neq k,..,  z_{i\rightarrow N}\neq k, z_{i\leftarrow N}\neq k)
\end{align*}

Again, it follows from the burstiness definition and Bayes' rule :
\begin{equation} \label{eq:memb_ex}
\frac{P(N_k^i=n+1)}{P(N_k^i=n)} = \frac{N-n}{n+1}P(z_{i\rightarrow j}=k,z_{i\leftarrow j}=k | Z_{i\rightarrow}^{-j}, Z_{i\leftarrow}^{-j})	
\end{equation}

The right part of equation \eqref{eq:memb_ex} has known closed form similar to the collaped Gibbs update for the membership assignment of IMMSB (see annexe). The burstiness equation reduce to :

\begin{align*}
\frac{P(N_k^i=n+1)}{P(N_k^i=n)} &= \frac{N-n}{n+1}P(z_{i\rightarrow j}=k | Z_{i\rightarrow}^{-j},) P(z_{i\leftarrow j}=k |  Z_{i\leftarrow}^{-j}) \\
&=  \frac{N-n}{n+1} \left( \frac{n + \alpha_k}{N + \alpha_{\bm{.}}}\right)^2
\end{align*}

\textcolor{red}{Here, there is this polynom of order 2 to solve to find the constraint on $\alpha$ to satisfy the burstiness, I skip it to win some times for now...}

 \end{proof}


\subsubsection{Local preferential attachment - General case}~\\

In the previous part we studied the preferential attachment in a context where we fit the model parameters. We showed that according to the type of prior (hard vs soft), it impacted the preferential attachment properties on the random network. To highlight this property we only needed to focus our analysis on the distribution conditioned on what we called $M_e$, which is actually an estimation of the parameters $F$ and $\Phi$ noted $\hat F$ and $\hat \Phi$. Those learned parameters are drawn from an approximate posterior noted $\hat P(F, \phi | Y, M_g) \propto P(Y|F, \Phi)P(F|M_g)P(F|M_g)$ which came from the inference process such that $M_e = (\hat F, \hat \Phi) \sim \hat P(F, \phi | Y, M_g)$. Thus which approximates the true posterior and provide the following generative equations :

\begin{equation}
    p(Y | \hat F, \hat \Phi) = \prod_{i<j} f_i \Phi f_j^T
\end{equation}

We define the expected degree as :

\begin{equation}
d_{i}^{M_g} = \sum_{j\in V} p(y_{ij}=1 | M_g)
\end{equation}

It is straightforward that the expected (global) degree is constant for any nodes in the networks for exchangeable models since  the expected link prediction is constant with $N$, and so on for IMMSB and ILFM. 

Nevertheless one can ask about the  expected (local) degree inside a class. The expected local degrees ($M_g$ case) given a class, is trivially given from the graphical models and correspond the following quantities  :
\begin{itemize}
	\item $\sum_{j\in V} p(y_{ij}=1  | z_{ij, ij}=k, M_g) = N \lambda_1 / (\lambda_1 + \lambda_0)$ for IMMSB,
	\item $\sum_{j\in V} p(y_{ij}=1  | f_{ik, jk}=1, M_g)$  for ILFM. Note that we can't compute it in a closed form expression for the same reason that previously of non-conjugacy.
\end{itemize}

In the general case, if we don't know the class a priori, the local degree expectation take the following form :

\begin{itemize}
\item  $d_{i,k}^{M_g} = \sum_{j\in V} p(y_{ij}=1, z_{ij, ij}=k | M_g)$ for IMMSB,
\item  $d_{i,k}^{M_g} = \sum_{j\in V} p(y_{ij}=1, f_{ik, jk}=1| M_g)$ for ILFM.
\end{itemize}

Note that it is easy to show we use this definition to define the degree in the $M_e$ context :
\begin{align*}
 d_{i,k}^{M_e} &= \sum_{j\in V} p(y_{ij}=1, z{i\rightarrow j}=k, z_{i\leftarrow j =k} | M_e) \\
  &= L(\phi_{kk} f_{ik} f{jk})
 \end{align*}
 
With the function $L()$ being the sigmoid for ILFM and the identity for IMMSB.

In turns out that again, this expected local degree have a closed form for (i)MMSB, and not for ILFM. For IMMSB we have the following :

\begin{align*}
p(y_{ij}=1, z_{ij, ij}=k | M_g) &= p(y_{ij}=1 |  z_{ij, ij}=k, M_g) p( z_{ij, ij}=k | M_g) \\
 &=E_{\phi_{kk}}[ p(y_{ij}=1 |  z_{ij, ij}=k, \phi_{kk}, M_g) ] E_{f_i}[ p( z_{ij}=k | f_i, M_g)] E_{f_j}[ p( z_{ji}=k | f_j, M_g)] \\
 &= \frac{\lambda_1}{\lambda_0+\lambda_1}\frac{ \alpha_k\alpha_k'}{ (\sum_k \alpha_k)^2}
\end{align*}

Note that from our definitions we have $d_i^{M_g} = \sum_k d_{i,k}^{M_g}$.


\paragraph{}
\textcolor{red}{Remark on the definition of the burstiness in the $M_g$ mode (expected degree):
\hrulefill
\hrulefill
~\\
}

for the burstiness in the "$M_g$" with de degree local defined as $d_{i,k}^{M_g}$  is trivially bursty, but makes no so much sense for the following reason :

The burstiness equation in this context can be written as :

\begin{equation}
b_n = \frac{p(d_{i,k}^{M_g} = n+1)}{p(d_{i,k}^{M_g} = n)}  \nearrow_? n 
\end{equation}

We know that $d_{i,k}^{M_g} = N \epsilon_k$ which is equivalent to says that $p(d_{i,k}^{M_g})$ is a Dirac distribution noted $\delta (N\epsilon_k)$. It means that the burstiness equation is only definite for $n = N\epsilon_k$, (otherwise the denominator is null), and hence its derivative is not defined.


\textcolor{red}{End fo Remark
\hrulefill
\hrulefill
~\\
}

The question of the local burstiness we should ask instead, is by seeking what are the sequence that underly a random variable that we want to analyze Here $d_{i,k}$ should not been seen as the sum over independent marginal probability, but as the joint probability of every draw concerned. 

In a first approach, let's assume we know the relative size of the class $k$ noted $N_i^k$:

\begin{align*}
P(d_{i,k}=n | M_g) &= \dbinom{N}{n} P(\{y_{ij}=1, z_{\{ij,ji\}}=k, j\in [1,n]\}, \\ & \{y_{ij}=0, z_{\{ij,ji\}}=k, j\in [1,N_i^k-n]\}, \\
&  \{\ z_{\{ij,ji\}}\neq k, j\in [1,N-N_i^k]\}) \\
\end{align*}


\textcolor{red}{Iam not sure about the number of combination here, I need to think about it.}

\section{Illustration}
\label{sec:exps}

To illustrate our theoretical results, we evaluate the predictive performance and the ability of the models to capture homophily and preferential attachment on artificial and real networks. For homophily, we simply compare the distributions of the natural and latent similarities on linked and non-linked pairs of nodes. For global preferential attachment, we use plots of the degree distribution and its corresponding best fitting line in log-log scale. In addition, we use the measure developed in \cite{clauset2009power} for assessing whether empirical data behaves according to a power law (as mentioned before, power laws are the standard bursty distributions in social networks \cite{barabasi1999emergence}). This framework combines maximum-likelihood methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistics to compute a $p$-value. If the obtained $p$-value is large (close to 1), then the data is likely to be distributed according to a power law and the associated network displays preferential attachment;  on the other hand, if it is small, the data is likely not distributed according to a power law and the associated network does not display preferential attachment.

For local preferential attachment, we follow the same approach as before to compute the $p$-value, the only difference being that the empirical data does not correspond any longer to the global adjacency matrix, but to reduced matrices for each class. The computation of the reduced adjacency matrices varies from one model to the other:
%
\begin{itemize}
    \item For \imb, for a given class $k$, the reduced adjacency matrix $Y^k$ is defined by:~$y_{ij,k}=1$ if $y_{ij}=1, z_{i\rightarrow j}=z_{i\leftarrow j}=k$ and $0$ otherwise.
    \item For \ifm, the reduced adjacency matrix $Y^k$ is defined by:~$ y_{ij,k}=1$ if $y_{ij}=1 , f_{ik}=f_{jk}=1$ and $0$ otherwise.
\end{itemize}
%

\subsection{Datasets and model parameters}

To illustrate the above developments, we consider two artificial and two real networks, the characteristics of which are summarized in Table~\ref{table:networks_measures}.

\input{table/net_measures}

The non-oriented artificial networks (Network1 and Network2) have been generated with the DANCer-Generator \cite{largeron2015}. This generator has been chosen because it allows one to build an attributed graph having a community structure as well as known properties of real-world networks such as preferential attachment and homophily. In order to test link prediction models on different types of networks, Network1 was generated, by design, to comply with preferential attachment whereas Network2 was not.

The first real network, denoted Blogs \footnote{moreno.ss.uci.edu/data.html\#blogs}, contains front-page hyperlinks between blogs in the context of the 2004 US election. A node represents a blog and an oriented link represents a hyperlink between two blogs. The second one, denoted Manufacturing \footnote{www.ii.pwr.edu.pl/~michalski/index.php?content=datasets\#manufacturing}, is an internal email communication network between employees of a mid-sized manufacturing company. Each node is associated to an employee and an oriented link represents an email sent between the two employees. One can notice that the second network is specific since it is an enterprise network in which the relationships between the employees are (professionally) constrained. This means that this network is less likely to display some of the properties that occur in unconstrained social networks.

The adjacency matrices and global degree distributions of these networks are presented in Figure \ref{fig:corpuses}. The adjacency matrices enable us to visualize some characteristics of the networks such as their density and their clustering patterns: as one can note, Blogs and the two artificial networks (Network1 and Network2) have a clear community structure, corresponding to the blocks of white dots on the figure, whereas Manufacturing, the denser network, does not have such a structure. Furthermore, the log-log scale plots show that Network1 and Blogs verify the  global preferential attachment (the fitted line represents relatively well the data points) whereas neither Network2 nor Manufacturing verify it. This is confirmed by the $p$-values reported in the first section of Table \ref{table:me_gofit} (Training Datasets): the $p$-value is 1 for Network1 and Blogs, whereas it is null for Network2 and Manufacturing. The parameter $\alpha$ reported in Table~\ref{table:me_gofit} corresponds to the parameter of the estimated power law distribution (\textit{i.e.} the slope of the best fitting line in log-log scale).

Figure \ref{fig:synt_graph_local} represents the local degree distributions for all networks, each curve in each plot being associated to a different class. As the ground truth is not available for the real networks (Blogs and Manufacturing), classes have been determined with Louvain algorithm \cite{Blondel2008} and the local distribution defined according to the obtained classes. As one can note, the plots for Network1 and Blogs are linear for the most frequent degrees, whereas the plots for Network2 and Manufacturing do not display any clear linearity, suggesting that Network1 and Blogs satisfy, at least partly, local preferential attachment whereas Network2 and Manufacturing do not. This is confirmed by the $p$-values reported in Table~\ref{table:me_gofit}: the $p$-value equals  $1$ for Network1 and Blogs,  $0$ for Network2 and $0.4$ for Manufacturing.

\begin{figure}[h]
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=1.1\textwidth]{img/corpus/network1_dd}
        \end{minipage}
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=1.1\textwidth]{img/corpus/network2_dd}
        \end{minipage}
        %\vskip\baselineskip
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=1.1\textwidth]{img/corpus/blogs_dd}
        \end{minipage}
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=1.1\textwidth]{img/corpus/manufacturing_dd}
        \end{minipage}
	\caption{Adjacency matrices (left) and global degree distributions (right) for the four training datasets. In the adjacency matrices, a white dot corresponds to a 1 and a block dot to a 0.}
	\label{fig:corpuses}
\end{figure}

\begin{figure}[h]
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/network1_1}
        \end{minipage}
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/network2_1}
        \end{minipage}
        \vskip\baselineskip
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/blogs_1}
        \end{minipage}
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/manufacturing_1}
        \end{minipage}
        \caption {Local degree distributions for the four training datasets. For Network1 and Network2 the classes come from ground-truth. For Blogs and Manufacturing, classes are obtained by Louvain algorithm.} 
	\label{fig:synt_graph_local}
\end{figure}

For each dataset, we estimate the model parameters through Markov Chain Monte Carlo inference consisting of 200 iterations. For \imb, the concentration parameters of HDP were optimized  using vague gamma priors $\alpha_0 \sim \text{Gamma}(1,1)$ and $\gamma \sim \text{Gamma}(1,1)$ following \cite{HDP}. The parameters for the matrix weights  $\lambda_0$ and $\lambda_1$ were fixed to 0.1. For \ifm, the hyperparameter  $\sigma_w$ was fixed to 1 and the IBP hyperparameter $\alpha$ to 0.5 in order to  have comparable number of classes with \imb. Once the models have been learned, they are used to generate links (or non-links) between the entire set of network nodes. The whole procedure is repeated 10 times and the average values are reported as final results.

\subsection{Homophily}

Figure \ref{fig:homo_mustach} presents boxplots describing the distributions of the natural $s_n(i,j)$ and latent $s_l(i,j)$ similarities computed respectively on linked and non-linked pairs of nodes for \imb\ (left) and \ifm\ (right). The results have been aggregated over the four datasets. They confirm that the natural similarity is  higher for pairs of nodes which are linked than for pairs of nodes which are not linked, for both models. For the latent similarity,  there is no difference between the linked and non-linked pairs, indicating that the links are not homophilic. These experimental results are in line with the theoretical results presented in Section~\ref{sec:homophily} that state that both \ifm\ and \imb\ are homophilic for  the natural similarity but are not homophilic for the latent similarity.

\begin{figure}[ht]
\centering
    \begin{subfigure}
       	 \centering
        	 \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/homo_mustach_immsb}
    \end{subfigure}
    \begin{subfigure}
        	 \centering
          \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/homo_mustach_ilfm}
    \end{subfigure}
    \caption{Natural and latent similarities aggregated over all datasets and computed on linked and non-linked pairs of nodes for \imb\ (left) and \ifm\ (right).}
    \label{fig:homo_mustach}
\end{figure}

\subsection{Preferential attachment}

Table \ref{table:me_gofit} reports the value of the power-law goodness of fit for \imb\ and \ifm\ in the global case (left) and in the local case (right). It appears that for both models, the global preferential attachment is only verified for networks generated from datasets where the property was observed, namely in Network1 with p-value equal to 0.9 for \imb\ and 1 for \ifm, and in Blogs with a p-value equal to 1 for both models; the property is not verified in Network2 and in Manufacturing, where p-values are equal to 0. This is in accordance with Proposition 2.1 according to which both \ifm\ and \imb\ do not satisfy global preferential attachment. However, these models are able to capture this property if it exists in the training datasets.  Moreover, one can observe that, in the local case, \imb\ complies with the preferential attachment with $p$-values equal or close to 1 for the four networks, while \ifm\ obtained low p-values for the networks that were less locally bursty (respectively  0  for Network2 and 0.3 for Manufacturing). In addition, the power-law coefficients $\alpha$ are significantly greater for \imb\ than for \ifm, and specially for the bursty networks Network1 and Blogs.

Figure \ref{fig:me_local} illustrates the local preferential attachment for Network1 (top) and Network2 (bottom) estimated with \imb\ (left) and \ifm\ (right). The shape of the local degree distributions appears more linear for \imb\ and with more fluctuations for \ifm. This illustrates the fact that \ifm\ does not capture local preferential attachment whereas \imb\ does, as stated in Proposition 2.2. 

\input{table/preferential_attachment}

\begin{figure}[h]
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/immsb_network1_1}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/ilfm_network1_1}
    \end{minipage}
    \vskip\baselineskip
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/immsb_network2_1}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/ilfm_network2_1}
    \end{minipage}
    \caption {Local degree distributions for Network1 (top row) and Network2 (bottom row) generated with fitted models \imb\ (first column) and \ifm\ (second column).} 
\label{fig:me_local}
\end{figure}

\begin{figure}[h]
\centering
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/roc_network1_75_f}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=4.22cm,height=3.5cm]{img/corpus/roc_network2_75_f}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/corpus/testset_max_20.png}
    \end{minipage}
    \caption{Top: AUC-ROC curves for Network1 (left) and Network2 (right) with 75 percent of data used for learning. Bottom: Relative performance of \imb\ and \ifm\ according to the percentage of data used for testing, the rest being used for learning.} 
\label{fig:auc}
\end{figure}

Lastly, Figure \ref{fig:auc} compares the performance of the models for predicting new links using the Area Under the Curve (AUC) measure as a function of the training set size. In the bottom plot, the y-axis gives the relative performance defined as the difference of the AUC values for \imb\ and \ifm\ ($AUC_{\imb} - AUC_{\ifm}$) whereas the x-axis indicates the percentage of links randomly removed from the datasets and used as test examples. Hence, the number of training data decreases with the x-axis and a positive value on the y-axis indicates that \imb\ outperforms \ifm.  The relative performance corresponds to the difference of the MAX AUC values obtained for both models on the 10 inference experiences. The top plots illustrate a case where 75 percent of the data is used as test set and where \imb\ dominates \ifm\ on Network1 (left), and the opposite on Network2 (right).

In general, as shown in the bottom plot, \ifm\ obtains better performance than \imb. However, the relative predictive performance of \imb\  increases  when the quantity of training data decreases on bursty networks, whereas for non-bursty networks the results are the opposite: the performance of \ifm\ increases when the size of the learning dataset decreases. This is particularly visible for Network2. The results for Manufacturing are less marked, which is certainly due to the small size of this network, making the prediction less challenging.

The above behavior can be explained by the fact that \imb\ satisfies the local preferential attachment whereas \ifm\ does not: as links are randomly removed, one is more likely to remove links from large classes than from small ones; a \section{Appendix}
\label{sec:append}model that enforces local preferential attachment on bursty networks is thus more likely to reconstruct those removed links. This is what is happening on Network1 and Blogs for \imb. On the contrary, for non-bursty networks, a model enforcing local preferential attachment is penalized.


\section{Conclusion}

We have studied whether stochastic mixed membership models, such as \ifm\ and \imb\, can generate new links while satisfying properties frequently verified in real  social networks, namely homophily and preferential attachment. To do so, we have introduced formal definitions of these properties and have analyzed how these models behave according to those definitions. We have shown, in particular, that both models are \textit{homophilic} with the natural similarity that underlies them. Concerning preferential attachment, we have shown that stochastic mixed membership models do not comply with global preferential attachment. The situation is however more contrasted when the property is considered at the local level: \imb\ enforces local preferential attachment whereas \ifm\ does not.

These findings have been validated experimentally on two real and two artificial networks that have different degrees of global and local preferential attachment. An important, practical finding of our study is that \imb, usually considered of lesser "quality" than \ifm, can indeed yield better results on bursty networks (\textit{i.e.} networks with preferential attachment) when the number of training data is limited.
% This study also paves the way to new models enforcing global preferential attachment through a strict adherence to the definition we have provided.

\bibliographystyle{unsrt}
\bibliography{./a}

\appendix
\section{Appendix}
\label{sec:append}
\subsection{Collapsed Gibbs sampling updates for IMMSB}

We provide here the derivation of the updates of the IMMSB model, described in Section~\ref{sec:models}.

%From the definition of the model, one has: $\pr(z_{ij} = k \mid \mat{f}_i) = f_{ik}$.

%\textcolor{red}{Adrien, peux-tu donner la d\'erivation ? La forme actuelle n'est valable que pour MMSB.} 

%heeeere \alpha is \alpha_0

Inference for the IMMSB model by using the Collapse Gibbs sampler gives updates for class assignment $Z \in N\times N \times 2$ for each interactions $Y \in N\times N$. Thus for all pair of interaction (i,j) we jointly sample the classes $(z_{ij}, z_{ji})$ who implicitly, take the values $(k,k')$ :
\begin{align} \label{eq:cgs}
&\pr(z_{ij}, z_{ji} \mid Z^-, Y,  \mat{\beta}, \alpha, \mat{\lambda} )  \\
&\propto\pr(z_{ij}, z_{ji} \mid Z^-, \alpha,\mat{\beta}) \pr(y_{ij} \mid Y^{-ij},  Z^-,z_{ij}, z_{ji},  \mat{\lambda} ) \nonumber
\end{align}
The term $Z^-$ denote that both $z_{ij}$ and $z_{ji}$ are exclude from $Z$. We now treat the first term of equation \ref{eq:cgs}.  
\begin{align}
&\pr(z_{ij}, z_{ji} \mid Z^-, \alpha,\mat{\beta})\\
&\propto \pr(z_{i\rightarrow j} \mid \{z_{i\rightarrow j'}\}_{j'\neq j}, \{z_{j'\leftarrow i}\}_{j'=1}^N, \alpha,\mat{\beta}) \\
& \quad .  \pr(z_{i\leftarrow j} \mid \{z_{i'\leftarrow j}\}_{i'\neq i}, \{z_{j\leftarrow i'}\}_{i'=1}^N, \alpha,\mat{\beta}) \nonumber
\end{align}
Let's consider the density of $z_{i\rightarrow j}$:
\begin{align}
&\propto \pr(z_{i\rightarrow j} \mid \{z_{i\rightarrow j'}\}_{j'\neq j}, \{z_{j'\leftarrow i}\}_{j'=1}^N, \alpha,\mat{\beta})  \\
&\propto \int_{f_i} \pr(f_i \mid \mat{\beta}, \alpha) \pr(z_{ij} \mid f_i) \prod_{j'\neq j} \pr(z_{ij'} \mid f_i) \prod_{j' =  1}^N  \pr(z_{j' i} \mid f_i)  df_i \nonumber
\end{align}


Due to the an augmented representation of the Chinese Restaurant Franchise (CRF) with the Stick Breaking Process \cite{HDP}, the density of the features can be approximated by the following Dirichlet distribution;
\begin{equation}
f_i \mid \mat{\beta}, \alpha \sim Dir(\alpha \beta_1,..,\alpha\beta_K, \alpha\beta_{new})
\end{equation}
Where $\alpha\beta_{new}$ represent the contribution for sampling a new class. Since $\pr(z_{ij} \mid f_i)$ is drawn from a multinomial, the model is said to be conjugate and reduce to a simple closed form expression:
\begin{enumerate}
	\item If the class $k$ has already been observed:
	\begin{align}
	\pr(z_{ij} =k \mid .) &\propto N_{ik}^{-ij} + \alpha_0 \beta_k
	\label{eq:update-immsb}
	\end{align}
	\item In case of a new class $k_{new}$:
	\begin{align}
	\pr(z_{ij} =k_{new} \mid.) &\propto \alpha_0 \beta_{new} \nonumber   
	\end{align}
\end{enumerate}
Where  $N_{ik}$ is the count for node $i$ being assigned to class $k$. As we show that the equations are symmetric, sampling for $z_{ji}$ is straightforward.

~\\
Again, referring the CRF, the sampling of the tables configuration $\mat{m}$ is given by: 
\begin{equation}
\pr(m_{ik} \mid Z, \bm{m}^{-ik}, \mat{\beta} ) = \frac{\Gamma(\alpha_0 \beta_k)}{\Gamma(\alpha_0 \beta_k + n_{j\bm{.   }k})} s(n_{j\bm{.}k}, m) (\alpha_0 \beta_k)^m
\end{equation}
And, finnaly  $\mat{\beta}$ is obtained by:
\begin{equation}
\mat{\beta} \sim Dir(m_1,.., m_K, \gamma)  
\end{equation}
Where $s(n,m)$ is the unsigned Stirling number of the first kind.


~\\
Finally, when the markov chain reach the stationnary distribution, the models parameters $\M = \{\mat{\Phi}, \mat{F}\}$ can be recovered by averaging the topics assignement counts for each membership and each relation:
\begin{align}
&\hat f_{ik} =\frac{ N_{ik} + \alpha\beta_k}{ N + \sum_k\alpha_k }\\
&\hat \phi_c  = \frac{M_{c1} + \lambda_1}{M_{c\bm{.}} + \lambda_0 + \lambda_1}
\end{align}


The count for node $i$ being assigned to membership $k$ is $N_{ik}$. And the count of for all couple of classes $c=(k,k')$ being associated to relation $r$ is $M_{cr}$. Note that in our case, the relation $r$ take values in (0,1) accounting for link or non-link between two node.

\end{document}
