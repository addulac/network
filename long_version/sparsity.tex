\section{Sparsity}
\label{sec:sparsity}

The last property we study in this paper is the sparsity. Sparsity is the fact that most values in a dataset are empty. Whenever a feature space can be big, an object would end up with most of his features to be equal to zero. In networks, the sparsity is directly quantified by the density of the network which means that a few number of edges is actually observed compared to set of possible edges in the network proportional to $\dbinom{N}{2}$. An other typical example where sparsity appears, is in text collections, where a few words of the dictionary appears in each document.

%defnition of dense and sparse netwroks are given in Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures(Peter Orbanz and Daniel M. Roy) \\

%definition of exchangeability for networs first mention in this interesting paper: Modeling homophily and stochastic equivalence in symmetric relational data [Hoff]


For probabilistic models, the density for undirected networks of size $|V|=N$, is defined by the expectation of generating a link, which represents the average chance to observe a link between two nodes. Hence this quantity is, in the $M_g$ settings:
%\begin{equation}
%p(y_{ij}^{new}=1| |V|=N) =  \int_{\Theta} p(y_{ij}^{new}=1|\Theta) \frac{\sum_{n=0}^{\dbinom{N}{2}} p(d^N=n| \Theta)}{\int_{\Theta} \sum_{n=0}^{\dbinom{N}{2}} p(d^N=n | \Theta)) p(\Theta) d\Theta} p(\Theta) d\Theta
%\end{equation}
%Where  $p(d^N=n)$ is the probability of a random graph with $N$ vertices and having $n$ edges. Thus it refers to the density of the graph. We now define the set of  adjacency matrix $Y_r$ corresponding to the ensemble of graphs having $n$ edges (and $N$ vertices) $Y_r \in \{Y: d=n\}$. Because of the exchangeability of the models, each of those adjacency matrices have the same distribution under joint random permutation of rows and colomns, given $\Theta$, and the number of different graph that occurs are $\dbinom{N}{C(Y_r)}$, with $C(Y_r)$ is the number of nodes of $Y_r$ having a non empty degree:
%\begin{equation}
%p(y_{ij}^{new}=1| |V|=N) =  \int_{\Theta} p(y_{ij}^{new}=1|\Theta) \frac{\sum_{n=0}^{\dbinom{N}{2}} \sum_{Y_r \in \{Y: d=n\}}\dbinom{N}{C(Y_r)} p(Y_r| \Theta)}{\int_{\Theta} \sum_{n=0}^{\dbinom{N}{2}}  \sum_{Y_r \in \{Y: d=n\}}\dbinom{N}{C(Y_r)} p(Y_r | \Theta)) p(\Theta) d\Theta} p(\Theta) d\Theta
%\end{equation}


%Here, one can see that from excheangeability we can take out the sum over graph from the integral on the parameters. The density take though a similar form than equation  \eqref{eq:sum}. We have then that $\Delta_N p(y_{ij}^{new}=1| |V|=N) >0 $.

\begin{align*}
&p(y_{ij}^{new}=1| \mathcal{M}_g) = \int_{f_i} \int_{f_j} \int_{\Phi} p(f_i| \alpha ) p(f_j| \alpha)p(\Phi| \lambda) \\
&\times \sum_{k<k'} p(y_{ij} \mid \phi_{kk'})\ p(k\mid f_i)p(k'\mid f_j)df_i df_j d\Phi \\
&=  \sum_{k<k'} \int_{\Phi} \phi_{kk'} p(\Phi| \lambda) d\Phi \int_{f_i} f_{ik} p(f_i| \alpha )df_i \int_{f_j} f_{jk}  p(f_j| \alpha ) df_j \\
&= \sum_{k<k'} \E[B(\lambda_1, \lambda_0)] \E[Dir(\mat{\alpha} )]_k \E[Dir(\mat{\alpha})]_{k'} \\
&= \frac{\lambda_1}{\lambda_0+\lambda_1}\frac{\sum_{kk'} \alpha_k\alpha_k'}{(\sum_k \alpha_k)^2} = \epsilon
\end{align*}


We see that the density do not dependent of $N$, and thus the number of edge grows with $N^2$ since its expectation is  $\dbinom{N}{2} \epsilon$.

This result shows that the model generate dense networks because the model is misspecified. More precisely, this results can be generalize for all graph that have an assumption of exchangeability, and particularly this exchangeability assumptions is true for the two models we studied here, though less obvious for ILFM since IBP doesn't draw independent feature vector, but the nodes exchangeability is inherited from the features exchangeability of the IBP \cite{orbanz2015bayesian}. This strong result is a direct consequence of the Haldous-Houver representation theorem for exchangeable array which generalize the de Finetti theorem for exchangeable sequence \cite{orbanz2015bayesian}. 
