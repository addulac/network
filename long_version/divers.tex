
\section{reference for MMSB}

Among the few adaptions of HDP for networks, who generalized the MMSB, earlier work used the CRF to quantify degree of membership in small networks \cite{iMMSB}. A more recent work \cite{diMMSB} put the HDP in a dynamic framework to model the membership of node in networks consecutive times. 


\section{reference for ILFM}
The two seminal work for ILFM are \cite{ILFRM} developed for dyadic relation. And a more general model BMF \cite{BMF} for rectangular matrices. An adaptation of ILFM for non-negative weights in the interaction matrix has been done in \cite{IMRM}. The authors used Hamiltonian MCMC updates to learn those weights.

\section{a rajouter dans burstiness, Ã  la suite de la proposition pour le burstiness dans les features/class}

The models of interest draws their features from either the HDP or the IBP who are build using the conjugate property of exponential family, respectively Multinomial-Dirichlet and Bernoulli-Beta. The posterior distribution can be in both case collapsed, leading to the following gibbs updates:

\begin{align}
    \pr(f_{ik} = 1 \mid F^{-ik}) &\propto m^{-ik} \\
    \pr(z_{i\rightarrow j} =k \mid Z^{-ij}) &\propto N_{ik}^{-ij} + \alpha_0
\end{align}

The fact the collapse form of the posterior for a feature depend on the count of previously assigned features enable the burstiness behavior at the feature level.


\section{Nonparametric update...}

\subsection{ILFM}
The probability to have $k_i^{new}$ features is proportional to this density:

\begin{equation} \label{eq:1}
\pr(k_i^{new} \mid Y, F, \alpha) \propto \pr(Y \mid F^{new})\pr(k_i^{new} \mid \alpha)
\end{equation}

The probability of having $k_i^{new}$ features is drawn from a $\mathrm{Poisson(\frac{\alpha}{N})}$ distribution in the IPB process. However, we also need to sample the $\mathbf{w}_i$ weights associated with those features, and in our case, the density of weights are not conjugate of the likelihood. In consequence, we cannot integrate them out as explicitly assumed in the equation \eqref{eq:1}. We follow the approach of ~\cite{BMF} to jointly sample the new features and the weights using a Metropolis-Hasting method. Thanks to the  exchangeability of the IPB we only need to consider features unique to entity $i$ to either propose or reject new features. Therefore, we reference by a superscript $B$ the set of model parameters corresponding to unique features. A convenient choice of jumping distribution is:

\begin{equation} \label{eq:j_knew}
J(k_i^{new}, \mathbf{\phi}_i^{new} \mid k_i^B, \mathbf{\phi}_i^B) = \mathrm{Poisson}(k_i^{new} \mid \alpha) \mathcal{N}(\mathbf{\phi}_i^{new} \mid \sigma_w)
\end{equation}

The acceptance ratio can thus be written as:

\begin{equation}
r_{B\rightarrow new} = \frac{\pr(k_i^{new}, \mathbf{\phi}_i^{new} \mid Y, \Phi, F, \alpha) J(k_i^{B}, \mathbf{\phi}_i^{B} \mid k_i^{new}, \mathbf{\phi}_i^{new})}{\pr(k_i^B, \mathbf{\phi}_i^B \mid Y, \Phi, F, \alpha) J(k_i^{new}, \mathbf{\phi}_i^{new} \mid k_i^B, \mathbf{\phi}_i^B)}
\end{equation}

When replacing by equation \eqref{eq:1} and \eqref{eq:j_knew}, the acceptance ratio simplify to ratio of data likelihoods:

\begin{equation} \label{eq:r_knew} 
r_{B\rightarrow new} = \frac{\pr(Y \mid F^{new}, \Phi^{new})}{\pr(Y \mid F, \Phi)}
\end{equation}

\subsection{IMMSB}

In the latent class model, the collapsed gibbs sampling approach allow us to only sample the classes couple for each observations:

\begin{equation}
    \pr(z_{ji}=k, z_{ij}=k' \mid .) \propto \pr(z_{ji}=k \mid .) \pr(z_{ij}=k' \mid .) f_{(k,k')}^{-ji}(y_{ji})
\end{equation}


The class assignment updates for the node couple are:
\begin{align}
    \pr(z_{i\rightarrow j} =k \mid Z^{-ij}) &\propto N_{ik}^{-ij} + \alpha_0 \beta_k \\
    \pr(z_{i\leftarrow j} =k \mid Z^{-ij}) &\propto N_{jk}^{-ij} + \alpha_0 \beta_k
\end{align}

Additionaly, there is a non-null probability to create a new classes with probability:

\begin{align}
    \pr(z_{i\rightarrow j} =k \mid Z^{-ij}) &\propto \alpha_0 \beta_u \\
    \pr(z_{i\leftarrow j} =k \mid Z^{-ij}) &\propto \alpha_0 \beta_u \\
\end{align}

Note that $\mathbf{\beta}$ as well as and the tables configuration in the CRF is sampled as proposed ~\cite{HDP}.
